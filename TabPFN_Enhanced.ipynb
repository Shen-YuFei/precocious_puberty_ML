{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabPFN增强版：性早熟预测模型\n",
    "\n",
    "**使用TabPFN 6.0.6 + Extensions 0.2.2 进行全方位优化**\n",
    "\n",
    "本笔记本展示了TabPFN的多种增强技术：\n",
    "- 基础TabPFN模型（**支持50000样本**、**KV Cache加速**、**概率校准**）\n",
    "- 超参数优化（HPO）\n",
    "- Post-hoc集成（AutoTabPFN）\n",
    "- SHAP可解释性分析\n",
    "- **PDP部分依赖图分析**（新功能）\n",
    "- **ShapIQ特征交互分析**（新功能）\n",
    "- 特征选择优化\n",
    "- 改进的采样策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有库导入完成\n"
     ]
    }
   ],
   "source": [
    "# 设置环境变量\n",
    "import os\n",
    "\n",
    "os.environ[\"SCIPY_ARRAY_API\"] = \"1\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# TabPFN相关库\n",
    "from tabpfn import TabPFNClassifier\n",
    "from tabpfn_extensions.hpo import TunedTabPFNClassifier\n",
    "from tabpfn_extensions.post_hoc_ensembles.sklearn_interface import AutoTabPFNClassifier\n",
    "from tabpfn_extensions import interpretability\n",
    "from tabpfn_extensions.embedding import TabPFNEmbedding\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"SimHei\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "print(\"所有库导入完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 设置路径和参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机种子设置: 825\n",
      "输出目录: ./output/enhanced/\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"./output\", exist_ok=True)\n",
    "os.makedirs(\"./output/models\", exist_ok=True)\n",
    "os.makedirs(\"./output/enhanced\", exist_ok=True)\n",
    "\n",
    "RANDOM_SEED = 825\n",
    "np.random.seed(RANDOM_SEED)\n",
    "N_JOBS = -1\n",
    "\n",
    "print(f\"随机种子设置: {RANDOM_SEED}\")\n",
    "print(f\"输出目录: ./output/enhanced/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正常组: 8970 行, 早熟组: 10654 行\n"
     ]
    }
   ],
   "source": [
    "normal_data = pd.read_csv(\"./input/性早熟数据激发试验正常组_new.csv\")\n",
    "disease_data = pd.read_csv(\"./input/激发试验确诊性早熟组数据_new.csv\")\n",
    "\n",
    "normal_data[\"group\"] = \"N\"\n",
    "disease_data[\"group\"] = \"Y\"\n",
    "\n",
    "print(f\"正常组: {normal_data.shape[0]} 行, 早熟组: {disease_data.shape[0]} 行\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 数据类型处理和合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并后数据: 19624 行 x 40 列\n",
      "分组统计:\n",
      "group\n",
      "Y    10654\n",
      "N     8970\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat([normal_data, disease_data], axis=0, ignore_index=True)\n",
    "data[\"group\"] = data[\"group\"].astype(\"category\")\n",
    "print(f\"合并后数据: {data.shape[0]} 行 x {data.shape[1]} 列\")\n",
    "print(f\"分组统计:\\n{data['group'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 划分训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集: 13736 行, 验证集: 5888 行\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data = train_test_split(\n",
    "    data, test_size=0.3, stratify=data[\"group\"], random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"训练集: {train_data.shape[0]} 行, 验证集: {validation_data.shape[0]} 行\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用 38 个特征\n",
      "训练集正负样本: {1: 7457, 0: 6279}\n",
      "验证集正负样本: {1: 3197, 0: 2691}\n"
     ]
    }
   ],
   "source": [
    "exclude_cols = [\"group\", \"患者编号\", \"Unnamed: 0\"]\n",
    "feature_cols = [col for col in train_data.columns if col not in exclude_cols]\n",
    "\n",
    "X_train = train_data[feature_cols].copy()\n",
    "y_train = train_data[\"group\"].copy()\n",
    "X_validation = validation_data[feature_cols].copy()\n",
    "y_validation = validation_data[\"group\"].copy()\n",
    "\n",
    "y_train_binary = (y_train == \"Y\").astype(int)\n",
    "y_validation_binary = (y_validation == \"Y\").astype(int)\n",
    "\n",
    "print(f\"使用 {len(feature_cols)} 个特征\")\n",
    "print(f\"训练集正负样本: {y_train_binary.value_counts().to_dict()}\")\n",
    "print(f\"验证集正负样本: {y_validation_binary.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 数据类型转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据类型转换中...\n",
      "转换完成 - 训练集: {dtype('float64'): 38}\n"
     ]
    }
   ],
   "source": [
    "print(\"数据类型转换中...\")\n",
    "\n",
    "for col in feature_cols:\n",
    "    X_train[col] = pd.to_numeric(X_train[col], errors=\"coerce\")\n",
    "    X_validation[col] = pd.to_numeric(X_validation[col], errors=\"coerce\")\n",
    "\n",
    "print(f\"转换完成 - 训练集: {X_train.dtypes.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 数据预处理（缺失值填充）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据预处理开始...\n",
      "预处理完成！\n",
      "  原始特征数: 38\n",
      "  处理后特征数: 38\n",
      "  训练集样本: 13736\n",
      "  验证集样本: 5888\n",
      "  缺失值: 0 (应为0)\n"
     ]
    }
   ],
   "source": [
    "print(\"数据预处理开始...\")\n",
    "\n",
    "# 使用MissForest方法填充缺失值（IterativeImputer + RandomForest）\n",
    "# 利用特征间的非线性关系\n",
    "imputer = IterativeImputer(\n",
    "    estimator=RandomForestRegressor(\n",
    "        n_estimators=10,\n",
    "        max_depth=10,\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_SEED,\n",
    "    ),\n",
    "    max_iter=10,\n",
    "    random_state=RANDOM_SEED,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "print(\"使用MissForest方法（IterativeImputer + RandomForest）...\")\n",
    "X_train_processed = imputer.fit_transform(X_train)\n",
    "X_validation_processed = imputer.transform(X_validation)\n",
    "\n",
    "# 移除全为NaN的特征\n",
    "valid_features = ~np.isnan(X_train_processed).all(axis=0)\n",
    "X_train_processed = X_train_processed[:, valid_features]\n",
    "X_validation_processed = X_validation_processed[:, valid_features]\n",
    "\n",
    "# 更新特征列表\n",
    "feature_cols_processed = [\n",
    "    col for col, valid in zip(feature_cols, valid_features) if valid\n",
    "]\n",
    "\n",
    "print(f\"预处理完成！\")\n",
    "print(f\"  原始特征数: {X_train.shape[1]}\")\n",
    "print(f\"  处理后特征数: {X_train_processed.shape[1]}\")\n",
    "print(f\"  训练集样本: {X_train_processed.shape[0]}\")\n",
    "print(f\"  验证集样本: {X_validation_processed.shape[0]}\")\n",
    "print(f\"  缺失值: {np.isnan(X_train_processed).sum()} (应为0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 模型训练与优化\n",
    "\n",
    "## 9. 准备训练数据（分层采样）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用13736个训练样本\n",
      "训练样本: 13736, 特征数: 38\n"
     ]
    }
   ],
   "source": [
    "# TabPFN v2.5 支持最多50000样本，可以使用全部训练数据\n",
    "max_samples = 50000  # 从10000提升到50000\n",
    "\n",
    "if len(X_train_processed) > max_samples:\n",
    "    print(f\"训练集样本数({len(X_train_processed)})超过{max_samples}，进行分层采样\")\n",
    "\n",
    "    # 使用分层采样保持类别平衡\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, train_size=max_samples, random_state=RANDOM_SEED\n",
    "    )\n",
    "    for sample_idx, _ in sss.split(X_train_processed, y_train_binary):\n",
    "        X_train_sampled = X_train_processed[sample_idx]\n",
    "        y_train_sampled = y_train_binary.iloc[sample_idx].values\n",
    "\n",
    "    print(f\"采样后类别分布: {np.bincount(y_train_sampled)}\")\n",
    "else:\n",
    "    X_train_sampled = X_train_processed\n",
    "    y_train_sampled = y_train_binary.values\n",
    "    print(f\"使用{len(X_train_sampled)}个训练样本\")\n",
    "\n",
    "print(f\"训练样本: {len(X_train_sampled)}, 特征数: {X_train_sampled.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 模型1：基础TabPFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "训练基础TabPFN模型\n",
      "======================================================================\n",
      "  - 训练样本数: 13736\n",
      "\n",
      "开始训练...\n",
      "\n",
      "基础TabPFN性能 (带概率校准):\n",
      "  AUC: 0.9645\n",
      "  F1:  0.9102\n",
      "  ACC: 0.8981\n",
      "\n",
      "模型已保存: ./output/enhanced/models/tabpfn_basic.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"训练基础TabPFN模型\")\n",
    "print(\"=\" * 70)\n",
    "from tabpfn.inference_tuning import ClassifierTuningConfig\n",
    "\n",
    "tabpfn_basic = TabPFNClassifier(\n",
    "    n_estimators=32,\n",
    "    device=\"cuda\",\n",
    "    random_state=RANDOM_SEED,\n",
    "    # fit_mode=\"fit_with_cache\",\n",
    "    eval_metric=\"f1\",\n",
    "    tuning_config=ClassifierTuningConfig(\n",
    "        calibrate_temperature=True,  # 校准softmax温度\n",
    "        tune_decision_thresholds=True,  # 优化决策阈值\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"  - 训练样本数: {len(X_train_sampled)}\")\n",
    "print(\"\\n开始训练...\")\n",
    "\n",
    "tabpfn_basic.fit(X_train_sampled, y_train_sampled)\n",
    "\n",
    "y_pred_basic = tabpfn_basic.predict(X_validation_processed)\n",
    "y_pred_proba_basic = tabpfn_basic.predict_proba(X_validation_processed)[:, 1]\n",
    "\n",
    "auc_basic = roc_auc_score(y_validation_binary, y_pred_proba_basic)\n",
    "f1_basic = f1_score(y_validation_binary, y_pred_basic)\n",
    "acc_basic = accuracy_score(y_validation_binary, y_pred_basic)\n",
    "\n",
    "print(f\"\\n基础TabPFN性能 (带概率校准):\")\n",
    "print(f\"  AUC: {auc_basic:.4f}\")\n",
    "print(f\"  F1:  {f1_basic:.4f}\")\n",
    "print(f\"  ACC: {acc_basic:.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "joblib.dump(\n",
    "    {\"model\": tabpfn_basic, \"imputer\": imputer},\n",
    "    \"./output/enhanced/models/tabpfn_basic.pkl\",\n",
    ")\n",
    "print(f\"\\n模型已保存: ./output/enhanced/models/tabpfn_basic.pkl\")\n",
    "# 基础TabPFN性能 (带概率校准):\n",
    "#   AUC: 0.9645\n",
    "#   F1:  0.9102\n",
    "#   ACC: 0.8981"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 模型2：超参数优化（HPO）\n",
    "\n",
    "使用TunedTabPFNClassifier自动搜索最优超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 666/700 [2:58:07<07:49, 13.80s/trial, best loss: -0.8944669603215551]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tabpfn_extensions.hpo.tuned_tabpfn:Trial failed with error: scikit-learn estimators should always specify their parameters in the signature of their __init__ (no varargs). <class 'tabpfn.preprocessors.kdi_transformer.KDITransformerWithNaN'> with constructor (self, *args: 'Any', **kwargs: 'Any') -> 'None' doesn't  follow this convention.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 676/700 [3:01:27<11:42, 29.27s/trial, best loss: -0.8944669603215551]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tabpfn_extensions.hpo.tuned_tabpfn:Trial failed with error: scikit-learn estimators should always specify their parameters in the signature of their __init__ (no varargs). <class 'tabpfn.preprocessors.kdi_transformer.KDITransformerWithNaN'> with constructor (self, *args: 'Any', **kwargs: 'Any') -> 'None' doesn't  follow this convention.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 684/700 [3:03:38<05:01, 18.86s/trial, best loss: -0.8944669603215551]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tabpfn_extensions.hpo.tuned_tabpfn:Trial failed with error: scikit-learn estimators should always specify their parameters in the signature of their __init__ (no varargs). <class 'tabpfn.preprocessors.kdi_transformer.KDITransformerWithNaN'> with constructor (self, *args: 'Any', **kwargs: 'Any') -> 'None' doesn't  follow this convention.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 687/700 [3:04:06<02:57, 13.62s/trial, best loss: -0.8944669603215551]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tabpfn_extensions.hpo.tuned_tabpfn:Trial failed with error: scikit-learn estimators should always specify their parameters in the signature of their __init__ (no varargs). <class 'tabpfn.preprocessors.kdi_transformer.KDITransformerWithNaN'> with constructor (self, *args: 'Any', **kwargs: 'Any') -> 'None' doesn't  follow this convention.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 688/700 [3:04:06<01:55,  9.61s/trial, best loss: -0.8944669603215551]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tabpfn_extensions.hpo.tuned_tabpfn:Trial failed with error: scikit-learn estimators should always specify their parameters in the signature of their __init__ (no varargs). <class 'tabpfn.preprocessors.kdi_transformer.KDITransformerWithNaN'> with constructor (self, *args: 'Any', **kwargs: 'Any') -> 'None' doesn't  follow this convention.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 690/700 [3:04:40<02:27, 14.78s/trial, best loss: -0.8944669603215551]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tabpfn_extensions.hpo.tuned_tabpfn:Trial failed with error: scikit-learn estimators should always specify their parameters in the signature of their __init__ (no varargs). <class 'tabpfn.preprocessors.kdi_transformer.KDITransformerWithNaN'> with constructor (self, *args: 'Any', **kwargs: 'Any') -> 'None' doesn't  follow this convention.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 692/700 [3:04:59<01:43, 12.97s/trial, best loss: -0.8944669603215551]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tabpfn_extensions.hpo.tuned_tabpfn:Trial failed with error: scikit-learn estimators should always specify their parameters in the signature of their __init__ (no varargs). <class 'tabpfn.preprocessors.kdi_transformer.KDITransformerWithNaN'> with constructor (self, *args: 'Any', **kwargs: 'Any') -> 'None' doesn't  follow this convention.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [3:07:43<00:00, 16.09s/trial, best loss: -0.8944669603215551]\n",
      "\n",
      "HPO优化TabPFN性能:\n",
      "  AUC: 0.9654 (vs 基础: +0.0010)\n",
      "  F1:  0.9108 (vs 基础: +0.0006)\n",
      "  ACC: 0.8989 (vs 基础: +0.0008)\n",
      "\n",
      "模型已保存: ./output/enhanced/models/tabpfn_hpo.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"训练HPO优化TabPFN模型\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 超参数优化\n",
    "tabpfn_hpo = TunedTabPFNClassifier(\n",
    "    n_trials=700,\n",
    "    metric=\"f1\",  # 优化目标：F1分数\n",
    "    # metric='roc_auc',       # 优化目标：AUC分数\n",
    "    device=\"cuda\",\n",
    "    random_state=RANDOM_SEED,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"开始超参数搜索...\")\n",
    "tabpfn_hpo.fit(X_train_sampled, y_train_sampled)\n",
    "\n",
    "y_pred_hpo = tabpfn_hpo.predict(X_validation_processed)\n",
    "y_pred_proba_hpo = tabpfn_hpo.predict_proba(X_validation_processed)[:, 1]\n",
    "\n",
    "auc_hpo = roc_auc_score(y_validation_binary, y_pred_proba_hpo)\n",
    "f1_hpo = f1_score(y_validation_binary, y_pred_hpo)\n",
    "acc_hpo = accuracy_score(y_validation_binary, y_pred_hpo)\n",
    "\n",
    "print(f\"\\nHPO优化TabPFN性能:\")\n",
    "print(f\"  AUC: {auc_hpo:.4f}\")\n",
    "print(f\"  F1:  {f1_hpo:.4f}\")\n",
    "print(f\"  ACC: {acc_hpo:.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "joblib.dump(\n",
    "    {\"model\": tabpfn_hpo, \"imputer\": imputer},\n",
    "    \"./output/enhanced/models/tabpfn_hpo.pkl\",\n",
    ")\n",
    "print(f\"\\n模型已保存: ./output/enhanced/models/tabpfn_hpo.pkl\")\n",
    "# HPO优化TabPFN性能:\n",
    "#   AUC: 0.9654\n",
    "#   F1:  0.9108\n",
    "#   ACC: 0.8989"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 模型3：Post-hoc集成（AutoTabPFN）\n",
    "\n",
    "自动训练和集成多个TabPFN配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "训练Post-hoc集成TabPFN模型\n",
      "======================================================================\n",
      "PyTorch CUDA 可用: True\n",
      "GPU 设备: NVIDIA GeForce RTX 3080 Laptop GPU\n",
      "CUDA 设备数量: 1\n",
      "Ray 版本: 2.6.0\n",
      "模型初始化成功\n",
      "使用设备: cuda:0\n",
      "训练样本数: 13736 (已启用ignore_pretraining_limits)\n",
      "开始自动集成训练...\n",
      "(这可能需要较长时间，正在训练多个模型配置...)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m开始自动集成训练...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m(这可能需要较长时间，正在训练多个模型配置...)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43mtabpfn_auto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_sampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_sampled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m y_pred_auto = tabpfn_auto.predict(X_validation_processed)\n\u001b[32m     39\u001b[39m y_pred_proba_auto = tabpfn_auto.predict_proba(X_validation_processed)[:, \u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\tabpfn_common_utils\\telemetry\\core\\decorators.py:185\u001b[39m, in \u001b[36m_wrap_callable_with_extension.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _extension_context(extension_name):\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Git-repository\\tabpfn-extensions\\src\\tabpfn_extensions\\post_hoc_ensembles\\sklearn_interface.py:410\u001b[39m, in \u001b[36mAutoTabPFNClassifier.fit\u001b[39m\u001b[34m(self, X, y, categorical_feature_indices, feature_names)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;66;03m# Normal case - multiple classes with sufficient samples per class\u001b[39;00m\n\u001b[32m    409\u001b[39m \u001b[38;5;28mself\u001b[39m.single_class_ = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_encoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Git-repository\\tabpfn-extensions\\src\\tabpfn_extensions\\post_hoc_ensembles\\sklearn_interface.py:267\u001b[39m, in \u001b[36mAutoTabPFNBase.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device_.type == \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    265\u001b[39m     num_gpus = torch.cuda.device_count()\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpresets\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpresets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_predictor_fit_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[38;5;66;03m# Set sklearn required attributes from the fitted predictor\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[38;5;28mself\u001b[39m.n_features_in_ = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.predictor_.features())\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\core\\utils\\decorators.py:31\u001b[39m, in \u001b[36munpack.<locals>._unpack_inner.<locals>._call\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(f)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call\u001b[39m(*args, **kwargs):\n\u001b[32m     30\u001b[39m     gargs, gkwargs = g(*other_args, *args, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1344\u001b[39m, in \u001b[36mTabularPredictor.fit\u001b[39m\u001b[34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, fit_full_last_level_weighted_ensemble, full_weighted_ensemble_additionally, dynamic_stacking, calibrate_decision_threshold, num_cpus, num_gpus, fit_strategy, memory_limit, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dynamic_stacking:\n\u001b[32m   1339\u001b[39m     logger.log(\n\u001b[32m   1340\u001b[39m         \u001b[32m20\u001b[39m,\n\u001b[32m   1341\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDyStack is enabled (dynamic_stacking=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdynamic_stacking\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m). \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1342\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1343\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1344\u001b[39m     num_stack_levels, time_limit = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dynamic_stacking\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mds_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1345\u001b[39m     logger.info(\n\u001b[32m   1346\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting main fit with num_stack_levels=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_stack_levels\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1347\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33mFor future fit calls on this dataset, you can skip DyStack to save time: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1348\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`predictor.fit(..., dynamic_stacking=False, num_stack_levels=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_stack_levels\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1349\u001b[39m     )\n\u001b[32m   1351\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (time_limit <= \u001b[32m0\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1444\u001b[39m, in \u001b[36mTabularPredictor._dynamic_stacking\u001b[39m\u001b[34m(self, ag_fit_kwargs, ag_post_fit_kwargs, validation_procedure, detection_time_frac, holdout_frac, n_folds, n_repeats, memory_safe_fits, clean_up_fits, enable_ray_logging, enable_callbacks, holdout_data)\u001b[39m\n\u001b[32m   1441\u001b[39m         _, holdout_data, _, _ = \u001b[38;5;28mself\u001b[39m._validate_fit_data(train_data=X, tuning_data=holdout_data)\n\u001b[32m   1442\u001b[39m         ds_fit_kwargs[\u001b[33m\"\u001b[39m\u001b[33mds_fit_context\u001b[39m\u001b[33m\"\u001b[39m] = os.path.join(ds_fit_context, \u001b[33m\"\u001b[39m\u001b[33msub_fit_custom_ho\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1444\u001b[39m     stacked_overfitting = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sub_fit_memory_save_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1446\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1447\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1448\u001b[39m \u001b[43m        \u001b[49m\u001b[43mds_fit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[43m        \u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_ag_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1450\u001b[39m \u001b[43m        \u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_ag_post_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1451\u001b[39m \u001b[43m        \u001b[49m\u001b[43mholdout_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mholdout_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1452\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1453\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1454\u001b[39m     \u001b[38;5;66;03m# Holdout is false, use (repeated) cross-validation\u001b[39;00m\n\u001b[32m   1455\u001b[39m     is_stratified = \u001b[38;5;28mself\u001b[39m.problem_type \u001b[38;5;129;01min\u001b[39;00m [BINARY, MULTICLASS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1648\u001b[39m, in \u001b[36mTabularPredictor._sub_fit_memory_save_wrapper\u001b[39m\u001b[34m(self, train_data, time_limit, time_start, ds_fit_kwargs, ag_fit_kwargs, ag_post_fit_kwargs, holdout_data)\u001b[39m\n\u001b[32m   1645\u001b[39m     normal_fit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1647\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m normal_fit:\n\u001b[32m-> \u001b[39m\u001b[32m1648\u001b[39m     stacked_overfitting, ho_leaderboard, exception = \u001b[43m_dystack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1650\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mds_fit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mholdout_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mholdout_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1658\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1659\u001b[39m     logger.log(\u001b[32m40\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWarning: Exception encountered during DyStack sub-fit:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexception\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:5851\u001b[39m, in \u001b[36m_dystack\u001b[39m\u001b[34m(predictor, train_data, time_limit, ds_fit_kwargs, ag_fit_kwargs, ag_post_fit_kwargs, holdout_data)\u001b[39m\n\u001b[32m   5849\u001b[39m logger.log(\u001b[32m20\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning DyStack sub-fit ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5850\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5851\u001b[39m     \u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5852\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   5853\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, e\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1369\u001b[39m, in \u001b[36mTabularPredictor._fit\u001b[39m\u001b[34m(self, ag_fit_kwargs, ag_post_fit_kwargs)\u001b[39m\n\u001b[32m   1367\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, ag_fit_kwargs: \u001b[38;5;28mdict\u001b[39m, ag_post_fit_kwargs: \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m   1368\u001b[39m     \u001b[38;5;28mself\u001b[39m.save(silent=\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Save predictor to disk to enable prediction and training after interrupt\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1369\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_learner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1370\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_post_fit_vars()\n\u001b[32m   1371\u001b[39m     \u001b[38;5;28mself\u001b[39m._post_fit(**ag_post_fit_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:159\u001b[39m, in \u001b[36mAbstractTabularLearner.fit\u001b[39m\u001b[34m(self, X, X_val, **kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mLearner is already fit.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    158\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_fit_input(X=X, X_val=X_val, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\tabular\\learner\\default_learner.py:133\u001b[39m, in \u001b[36mDefaultLearner._fit\u001b[39m\u001b[34m(self, X, X_val, X_test, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, raise_on_model_failure, **trainer_fit_kwargs)\u001b[39m\n\u001b[32m    130\u001b[39m     \u001b[38;5;28mself\u001b[39m.eval_metric = trainer.eval_metric\n\u001b[32m    132\u001b[39m \u001b[38;5;28mself\u001b[39m.save()\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mholdout_frac\u001b[49m\u001b[43m=\u001b[49m\u001b[43mholdout_frac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit_trainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrainer_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28mself\u001b[39m.save_trainer(trainer=trainer)\n\u001b[32m    149\u001b[39m time_end = time.time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\tabular\\trainer\\auto_trainer.py:135\u001b[39m, in \u001b[36mAutoTrainer.fit\u001b[39m\u001b[34m(self, X, y, hyperparameters, X_val, y_val, X_test, y_test, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, aux_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, callbacks, **kwargs)\u001b[39m\n\u001b[32m    132\u001b[39m log_str += \u001b[33m\"\u001b[39m\u001b[33m}\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m logger.log(\u001b[32m20\u001b[39m, log_str)\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_multi_and_ensemble\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_stack_levels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_stack_levels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcore_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcore_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43maux_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43maux_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py:3313\u001b[39m, in \u001b[36mAbstractTabularTrainer._train_multi_and_ensemble\u001b[39m\u001b[34m(self, X, y, X_val, y_val, X_test, y_test, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001b[39m\n\u001b[32m   3311\u001b[39m     \u001b[38;5;28mself\u001b[39m._num_rows_test = \u001b[38;5;28mlen\u001b[39m(X_test)\n\u001b[32m   3312\u001b[39m \u001b[38;5;28mself\u001b[39m._num_cols_train = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(X.columns))\n\u001b[32m-> \u001b[39m\u001b[32m3313\u001b[39m model_names_fit = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_multi_levels\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3314\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3315\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3316\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3317\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3318\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3320\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel_start\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel_end\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_stack_levels\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3325\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3326\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.get_model_names()) == \u001b[32m0\u001b[39m:\n\u001b[32m   3328\u001b[39m     \u001b[38;5;66;03m# TODO v1.0: Add toggle to raise exception if no models trained\u001b[39;00m\n\u001b[32m   3329\u001b[39m     logger.log(\u001b[32m30\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mWarning: AutoGluon did not successfully train any models\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py:506\u001b[39m, in \u001b[36mAbstractTabularTrainer.train_multi_levels\u001b[39m\u001b[34m(self, X, y, hyperparameters, X_val, y_val, X_test, y_test, X_unlabeled, base_model_names, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack, level_time_modifier, infer_limit, infer_limit_batch_size, callbacks)\u001b[39m\n\u001b[32m    504\u001b[39m         core_kwargs_level[\u001b[33m\"\u001b[39m\u001b[33mtime_limit\u001b[39m\u001b[33m\"\u001b[39m] = core_kwargs_level.get(\u001b[33m\"\u001b[39m\u001b[33mtime_limit\u001b[39m\u001b[33m\"\u001b[39m, time_limit_core)\n\u001b[32m    505\u001b[39m         aux_kwargs_level[\u001b[33m\"\u001b[39m\u001b[33mtime_limit\u001b[39m\u001b[33m\"\u001b[39m] = aux_kwargs_level.get(\u001b[33m\"\u001b[39m\u001b[33mtime_limit\u001b[39m\u001b[33m\"\u001b[39m, time_limit_aux)\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m     base_model_names, aux_models = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstack_new_level\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbase_model_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_model_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcore_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcore_kwargs_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43maux_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43maux_kwargs_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname_suffix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname_suffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfull_weighted_ensemble\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_weighted_ensemble\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_full_weighted_ensemble\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_full_weighted_ensemble\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    525\u001b[39m     model_names_fit += base_model_names + aux_models\n\u001b[32m    526\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.model_best \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m infer_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model_names_fit) != \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py:706\u001b[39m, in \u001b[36mAbstractTabularTrainer.stack_new_level\u001b[39m\u001b[34m(self, X, y, models, X_val, y_val, X_test, y_test, X_unlabeled, level, base_model_names, core_kwargs, aux_kwargs, name_suffix, infer_limit, infer_limit_batch_size, full_weighted_ensemble, additional_full_weighted_ensemble)\u001b[39m\n\u001b[32m    704\u001b[39m     core_kwargs[\u001b[33m\"\u001b[39m\u001b[33mname_suffix\u001b[39m\u001b[33m\"\u001b[39m] = core_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mname_suffix\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) + name_suffix\n\u001b[32m    705\u001b[39m     aux_kwargs[\u001b[33m\"\u001b[39m\u001b[33mname_suffix\u001b[39m\u001b[33m\"\u001b[39m] = aux_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mname_suffix\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) + name_suffix\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m core_models = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstack_new_level_core\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_model_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_model_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcore_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    722\u001b[39m aux_models = []\n\u001b[32m    723\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m full_weighted_ensemble:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py:857\u001b[39m, in \u001b[36mAbstractTabularTrainer.stack_new_level_core\u001b[39m\u001b[34m(self, X, y, models, X_val, y_val, X_test, y_test, X_unlabeled, level, base_model_names, fit_strategy, stack_name, ag_args, ag_args_fit, ag_args_ensemble, included_model_types, excluded_model_types, ensemble_type, name_suffix, get_models_func, refit_full, infer_limit, infer_limit_batch_size, **kwargs)\u001b[39m\n\u001b[32m    851\u001b[39m fit_kwargs = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m    852\u001b[39m     num_classes=\u001b[38;5;28mself\u001b[39m.num_classes,\n\u001b[32m    853\u001b[39m     feature_metadata=feature_metadata,\n\u001b[32m    854\u001b[39m )\n\u001b[32m    856\u001b[39m \u001b[38;5;66;03m# FIXME: TODO: v0.1 X_unlabeled isn't cached so it won't be available during refit_full or fit_extra.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m857\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_multi\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstack_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstack_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfit_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfit_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py:3245\u001b[39m, in \u001b[36mAbstractTabularTrainer._train_multi\u001b[39m\u001b[34m(self, X, y, models, hyperparameter_tune_kwargs, feature_prune_kwargs, k_fold, n_repeats, n_repeat_start, time_limit, delay_bag_sets, **kwargs)\u001b[39m\n\u001b[32m   3243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_repeat_start == \u001b[32m0\u001b[39m:\n\u001b[32m   3244\u001b[39m     time_start = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3245\u001b[39m     model_names_trained = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_multi_initial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3247\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk_fold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk_fold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_repeats_initial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_prune_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_prune_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3254\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3256\u001b[39m     n_repeat_start = n_repeats_initial\n\u001b[32m   3257\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py:2840\u001b[39m, in \u001b[36mAbstractTabularTrainer._train_multi_initial\u001b[39m\u001b[34m(self, X, y, models, k_fold, n_repeats, hyperparameter_tune_kwargs, time_limit, feature_prune_kwargs, **kwargs)\u001b[39m\n\u001b[32m   2838\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2839\u001b[39m     time_ratio = hpo_time_ratio \u001b[38;5;28;01mif\u001b[39;00m hpo_enabled \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2840\u001b[39m     models = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_multi_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk_fold_start\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk_fold_end\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk_fold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_repeat_start\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2848\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtime_split\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2849\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtime_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2850\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2851\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2853\u001b[39m multi_fold_time_elapsed = time.time() - multi_fold_time_start\n\u001b[32m   2854\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py:2997\u001b[39m, in \u001b[36mAbstractTabularTrainer._train_multi_fold\u001b[39m\u001b[34m(self, X, y, models, time_limit, time_split, time_ratio, hyperparameter_tune_kwargs, fit_strategy, **kwargs)\u001b[39m\n\u001b[32m   2994\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._callback_early_stop:\n\u001b[32m   2995\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m models_valid\n\u001b[32m-> \u001b[39m\u001b[32m2997\u001b[39m         models_valid += \u001b[43m_detached_train_multi_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2998\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_self\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2999\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3000\u001b[39m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3001\u001b[39m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3002\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3003\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_split\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3004\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3005\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_limit_model_split\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit_model_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3006\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3007\u001b[39m \u001b[43m            \u001b[49m\u001b[43mis_ray_worker\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3008\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3009\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3010\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m fit_strategy == \u001b[33m\"\u001b[39m\u001b[33mparallel\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3011\u001b[39m     models_valid = \u001b[38;5;28mself\u001b[39m._train_multi_fold_parallel(\n\u001b[32m   3012\u001b[39m         X=X,\n\u001b[32m   3013\u001b[39m         y=y,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3020\u001b[39m         **kwargs,\n\u001b[32m   3021\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py:4588\u001b[39m, in \u001b[36m_detached_train_multi_fold\u001b[39m\u001b[34m(_self, model, X, y, time_split, time_start, time_limit, time_limit_model_split, hyperparameter_tune_kwargs, is_ray_worker, kwargs)\u001b[39m\n\u001b[32m   4585\u001b[39m         time_start_model=time.time()\n\u001b[32m   4586\u001b[39m         time_left=time_limit-(time_start_model-time_start)\n\u001b[32m-> \u001b[39m\u001b[32m4588\u001b[39m model_name_trained_lst = \u001b[43m_self\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_train_single_full\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4589\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4590\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_left\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhyperparameter_tune_kwargs_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_ray_worker\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_ray_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4595\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   4596\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4598\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _self.low_memory:\n\u001b[32m   4599\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py:2613\u001b[39m, in \u001b[36mAbstractTabularTrainer._train_single_full\u001b[39m\u001b[34m(self, X, y, model, X_unlabeled, X_val, y_val, X_test, y_test, X_pseudo, y_pseudo, hyperparameter_tune_kwargs, stack_name, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, level, time_limit, fit_kwargs, compute_score, total_resources, errors, errors_ignore, errors_raise, is_ray_worker, **kwargs)\u001b[39m\n\u001b[32m   2609\u001b[39m         bagged_model_fit_kwargs = \u001b[38;5;28mself\u001b[39m._get_bagged_model_fit_kwargs(\n\u001b[32m   2610\u001b[39m             k_fold=k_fold, k_fold_start=k_fold_start, k_fold_end=k_fold_end, n_repeats=n_repeats, n_repeat_start=n_repeat_start\n\u001b[32m   2611\u001b[39m         )\n\u001b[32m   2612\u001b[39m         model_fit_kwargs.update(bagged_model_fit_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2613\u001b[39m     model_names_trained = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_and_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2614\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2615\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2616\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2617\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2618\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2620\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstack_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstack_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompute_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_resources\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_resources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2626\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2627\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors_ignore\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors_ignore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2628\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors_raise\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors_raise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_ray_worker\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_ray_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2630\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2631\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2632\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks \u001b[38;5;129;01mand\u001b[39;00m check_callbacks:\n\u001b[32m   2633\u001b[39m     \u001b[38;5;28mself\u001b[39m._callbacks_after_fit(model_names=model_names_trained, stack_name=stack_name, level=level)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py:2171\u001b[39m, in \u001b[36mAbstractTabularTrainer._train_and_save\u001b[39m\u001b[34m(self, X, y, model, X_val, y_val, X_test, y_test, X_pseudo, y_pseudo, time_limit, stack_name, level, compute_score, total_resources, errors, errors_ignore, errors_raise, is_ray_worker, **model_fit_kwargs)\u001b[39m\n\u001b[32m   2169\u001b[39m exception = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2170\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2171\u001b[39m     model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2173\u001b[39m     fit_end_time = time.time()\n\u001b[32m   2174\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.weight_evaluation:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py:2055\u001b[39m, in \u001b[36mAbstractTabularTrainer._train_single\u001b[39m\u001b[34m(self, X, y, model, X_val, y_val, X_test, y_test, total_resources, **model_fit_kwargs)\u001b[39m\n\u001b[32m   2039\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_train_single\u001b[39m(\n\u001b[32m   2040\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2041\u001b[39m     X: pd.DataFrame,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2049\u001b[39m     **model_fit_kwargs,\n\u001b[32m   2050\u001b[39m ) -> AbstractModel:\n\u001b[32m   2051\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2052\u001b[39m \u001b[33;03m    Trains model but does not add the trained model to this Trainer.\u001b[39;00m\n\u001b[32m   2053\u001b[39m \u001b[33;03m    Returns trained model object.\u001b[39;00m\n\u001b[32m   2054\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2055\u001b[39m     model = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_resources\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_resources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2056\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py:1068\u001b[39m, in \u001b[36mAbstractModel.fit\u001b[39m\u001b[34m(self, log_resources, **kwargs)\u001b[39m\n\u001b[32m   1066\u001b[39m         msg += msg_mem\n\u001b[32m   1067\u001b[39m     logger.log(\u001b[32m20\u001b[39m, msg)\n\u001b[32m-> \u001b[39m\u001b[32m1068\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1070\u001b[39m     out = \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py:270\u001b[39m, in \u001b[36mStackerEnsembleModel._fit\u001b[39m\u001b[34m(self, X, y, compute_base_preds, time_limit, **kwargs)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    269\u001b[39m     time_limit = time_limit - (time.time() - start_time)\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py:389\u001b[39m, in \u001b[36mBaggedEnsembleModel._fit\u001b[39m\u001b[34m(self, X, y, X_val, y_val, X_pseudo, y_pseudo, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, groups, _skip_oof, **kwargs)\u001b[39m\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# Reserve time for final refit model\u001b[39;00m\n\u001b[32m    388\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mtime_limit\u001b[39m\u001b[33m\"\u001b[39m] = kwargs[\u001b[33m\"\u001b[39m\u001b[33mtime_limit\u001b[39m\u001b[33m\"\u001b[39m] * folds_to_fit / (folds_to_fit + \u001b[32m1.2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_folds\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_pseudo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_pseudo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_pseudo\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_pseudo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_fold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk_fold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_fold_start\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk_fold_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_fold_end\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk_fold_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_repeat_start\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_repeat_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_folds\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_bag_folds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[38;5;66;03m# FIXME: Cleanup self\u001b[39;00m\n\u001b[32m    405\u001b[39m \u001b[38;5;66;03m# FIXME: Support `can_refit_full=False` models\u001b[39;00m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m refit_folds:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py:868\u001b[39m, in \u001b[36mBaggedEnsembleModel._fit_folds\u001b[39m\u001b[34m(self, X, y, model_base, X_pseudo, y_pseudo, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, time_limit, sample_weight, save_folds, groups, num_cpus, num_gpus, **kwargs)\u001b[39m\n\u001b[32m    866\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fold_fit_args \u001b[38;5;129;01min\u001b[39;00m fold_fit_args_list:\n\u001b[32m    867\u001b[39m     fold_fitting_strategy.schedule_fold_model_fit(**fold_fit_args)\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m \u001b[43mfold_fitting_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mafter_all_folds_scheduled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[38;5;66;03m# Do this to maintain model name order based on kfold split regardless of which model finished first in parallel mode\u001b[39;00m\n\u001b[32m    871\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fold_fit_args \u001b[38;5;129;01min\u001b[39;00m fold_fit_args_list:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py:350\u001b[39m, in \u001b[36mSequentialLocalFoldFittingStrategy.after_all_folds_scheduled\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mafter_all_folds_scheduled\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    349\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m job \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jobs:\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_fold_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py:356\u001b[39m, in \u001b[36mSequentialLocalFoldFittingStrategy._fit_fold_model\u001b[39m\u001b[34m(self, fold_ctx)\u001b[39m\n\u001b[32m    354\u001b[39m time_limit_fold = \u001b[38;5;28mself\u001b[39m._get_fold_time_limit(fold_ctx)\n\u001b[32m    355\u001b[39m fold_model = \u001b[38;5;28mself\u001b[39m._fit(\u001b[38;5;28mself\u001b[39m.model_base, time_start_fold, time_limit_fold, fold_ctx, \u001b[38;5;28mself\u001b[39m.model_base_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m fold_model, pred_proba = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_oof\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfold_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[38;5;28mself\u001b[39m._update_bagged_ensemble(fold_model, pred_proba, fold_ctx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py:278\u001b[39m, in \u001b[36mFoldFittingStrategy._predict_oof\u001b[39m\u001b[34m(self, fold_model, fold_ctx)\u001b[39m\n\u001b[32m    276\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m expected_remaining_time_required > time_left:\n\u001b[32m    277\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m TimeLimitExceeded\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m y_pred_proba = \u001b[43mfold_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord_time\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m fold_model.val_score = fold_model.score_with_y_pred_proba(y=y_val_fold, y_pred_proba=y_pred_proba)\n\u001b[32m    280\u001b[39m fold_model.reduce_memory_size(remove_fit=\u001b[38;5;28;01mTrue\u001b[39;00m, remove_info=\u001b[38;5;28;01mFalse\u001b[39;00m, requires_save=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py:1252\u001b[39m, in \u001b[36mAbstractModel.predict_proba\u001b[39m\u001b[34m(self, X, normalize, record_time, **kwargs)\u001b[39m\n\u001b[32m   1227\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1228\u001b[39m \u001b[33;03mReturns class prediction probabilities of X.\u001b[39;00m\n\u001b[32m   1229\u001b[39m \u001b[33;03mFor binary problems, this returns the positive class label probability as a 1d numpy array.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1248\u001b[39m \u001b[33;03m    The prediction probabilities\u001b[39;00m\n\u001b[32m   1249\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1250\u001b[39m time_start = time.time() \u001b[38;5;28;01mif\u001b[39;00m record_time \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1252\u001b[39m y_pred_proba = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_proba_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.params_aux.get(\u001b[33m\"\u001b[39m\u001b[33mtemperature_scalar\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1255\u001b[39m     y_pred_proba = \u001b[38;5;28mself\u001b[39m._apply_temperature_scaling(y_pred_proba)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py:1266\u001b[39m, in \u001b[36mAbstractModel._predict_proba_internal\u001b[39m\u001b[34m(self, X, normalize, **kwargs)\u001b[39m\n\u001b[32m   1264\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m normalize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1265\u001b[39m     normalize = \u001b[38;5;28mself\u001b[39m.normalize_pred_probas\n\u001b[32m-> \u001b[39m\u001b[32m1266\u001b[39m y_pred_proba = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1267\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[32m   1268\u001b[39m     y_pred_proba = normalize_pred_probas(y_pred_proba, \u001b[38;5;28mself\u001b[39m.problem_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py:1305\u001b[39m, in \u001b[36mAbstractModel._predict_proba\u001b[39m\u001b[34m(self, X, **kwargs)\u001b[39m\n\u001b[32m   1302\u001b[39m     y_pred = \u001b[38;5;28mself\u001b[39m.model.predict(X)\n\u001b[32m   1303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m y_pred.reshape([-\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.quantile_levels)])\n\u001b[32m-> \u001b[39m\u001b[32m1305\u001b[39m y_pred_proba = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1306\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._convert_proba_to_unified_form(y_pred_proba)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\tabpfn_common_utils\\telemetry\\core\\decorators.py:288\u001b[39m, in \u001b[36mtrack_model_call.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_safe_call_with_telemetry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_names\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\tabpfn_common_utils\\telemetry\\core\\decorators.py:332\u001b[39m, in \u001b[36m_safe_call_with_telemetry\u001b[39m\u001b[34m(func, args, kwargs, model_method, param_names)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# Step 2: Run the actual function\u001b[39;00m\n\u001b[32m    331\u001b[39m start = time.perf_counter()\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m duration_ms = \u001b[38;5;28mint\u001b[39m((time.perf_counter() - start) * \u001b[32m1000\u001b[39m)\n\u001b[32m    335\u001b[39m \u001b[38;5;66;03m# Step 3: Send telemetry event\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Git-repository\\TabPFN\\src\\tabpfn\\classifier.py:1089\u001b[39m, in \u001b[36mTabPFNClassifier.predict_proba\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m   1076\u001b[39m \u001b[38;5;129m@track_model_call\u001b[39m(model_method=\u001b[33m\"\u001b[39m\u001b[33mpredict\u001b[39m\u001b[33m\"\u001b[39m, param_names=[\u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1077\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: XType) -> np.ndarray:\n\u001b[32m   1078\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Predict the probabilities of the classes for the provided input samples.\u001b[39;00m\n\u001b[32m   1079\u001b[39m \n\u001b[32m   1080\u001b[39m \u001b[33;03m    This is a wrapper around the `_predict_proba` method.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1087\u001b[39m \u001b[33;03m        Shape (n_samples, n_classes).\u001b[39;00m\n\u001b[32m   1088\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1089\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\contextlib.py:81\u001b[39m, in \u001b[36mContextDecorator.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args, **kwds):\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recreate_cm():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Git-repository\\TabPFN\\src\\tabpfn\\classifier.py:1103\u001b[39m, in \u001b[36mTabPFNClassifier._predict_proba\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;129m@config_context\u001b[39m(transform_output=\u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_predict_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: XType) -> np.ndarray:\n\u001b[32m   1093\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Predict the probabilities of the classes for the provided input samples.\u001b[39;00m\n\u001b[32m   1094\u001b[39m \n\u001b[32m   1095\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1100\u001b[39m \u001b[33;03m        Shape (n_samples, n_classes).\u001b[39;00m\n\u001b[32m   1101\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1102\u001b[39m     probas = (\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m.float().detach().cpu().numpy()\n\u001b[32m   1104\u001b[39m     )\n\u001b[32m   1105\u001b[39m     probas = \u001b[38;5;28mself\u001b[39m._maybe_reweight_probas(probas=probas)\n\u001b[32m   1106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.inference_config_.USE_SKLEARN_16_DECIMAL_PRECISION:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Git-repository\\TabPFN\\src\\tabpfn\\classifier.py:1011\u001b[39m, in \u001b[36mTabPFNClassifier._raw_predict\u001b[39m\u001b[34m(self, X, return_logits, return_raw_logits)\u001b[39m\n\u001b[32m   1008\u001b[39m     X = fix_dtypes(X, cat_indices=\u001b[38;5;28mself\u001b[39m.inferred_categorical_indices_)\n\u001b[32m   1009\u001b[39m     X = process_text_na_dataframe(X, ord_encoder=\u001b[38;5;28mself\u001b[39m.preprocessor_)\n\u001b[32m-> \u001b[39m\u001b[32m1011\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_inference_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_raw_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_raw_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Git-repository\\TabPFN\\src\\tabpfn\\classifier.py:1330\u001b[39m, in \u001b[36mTabPFNClassifier.forward\u001b[39m\u001b[34m(self, X, use_inference_mode, return_logits, return_raw_logits)\u001b[39m\n\u001b[32m   1327\u001b[39m     \u001b[38;5;28mself\u001b[39m.executor_.use_torch_inference_mode(use_inference=use_inference_mode)\n\u001b[32m   1329\u001b[39m outputs = []\n\u001b[32m-> \u001b[39m\u001b[32m1330\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecutor_\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevices_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautocast\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muse_autocast_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m    \u001b[49m\u001b[43moriginal_ndim\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mndim\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# This block correctly handles both single configs and lists of configs\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Git-repository\\TabPFN\\src\\tabpfn\\inference.py:556\u001b[39m, in \u001b[36mInferenceEngineCachePreprocessing.iter_outputs\u001b[39m\u001b[34m(self, X, devices, autocast, only_return_standard_out)\u001b[39m\n\u001b[32m    540\u001b[39m model_forward_functions = (\n\u001b[32m    541\u001b[39m     partial(\n\u001b[32m    542\u001b[39m         \u001b[38;5;28mself\u001b[39m._call_model,\n\u001b[32m   (...)\u001b[39m\u001b[32m    552\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.ensemble_configs))\n\u001b[32m    553\u001b[39m )\n\u001b[32m    554\u001b[39m outputs = parallel_execute(devices, model_forward_functions)\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mensemble_configs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_move_and_squeeze_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mensemble_configs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.inference_mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Git-repository\\TabPFN\\src\\tabpfn\\parallel_execute.py:58\u001b[39m, in \u001b[36mparallel_execute\u001b[39m\u001b[34m(devices, functions)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Evaluate the given functions in parallel across `devices`.\u001b[39;00m\n\u001b[32m     40\u001b[39m \n\u001b[32m     41\u001b[39m \u001b[33;03mThe function evaluations are parallelised using Python threads, so this will only\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     54\u001b[39m \u001b[33;03m    as `functions`.\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(devices) == \u001b[32m1\u001b[39m:\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# If we only have one device then just use the current thread to avoid overhead.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _execute_in_current_thread(devices[\u001b[32m0\u001b[39m], functions)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _execute_with_multithreading(devices, functions)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Git-repository\\TabPFN\\src\\tabpfn\\parallel_execute.py:67\u001b[39m, in \u001b[36m_execute_in_current_thread\u001b[39m\u001b[34m(device, functions)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_execute_in_current_thread\u001b[39m(\n\u001b[32m     64\u001b[39m     device: torch.device, functions: Iterable[ParallelFunction[R_co]]\n\u001b[32m     65\u001b[39m ) -> Generator[R_co]:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m function \u001b[38;5;129;01min\u001b[39;00m functions:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_parallel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Git-repository\\TabPFN\\src\\tabpfn\\inference.py:599\u001b[39m, in \u001b[36mInferenceEngineCachePreprocessing._call_model\u001b[39m\u001b[34m(self, device, is_parallel, X_train, X_test, y_train, cat_ix, autocast, only_return_standard_out, model_index, save_peak_mem)\u001b[39m\n\u001b[32m    591\u001b[39m save_peak_memory_factor = (\n\u001b[32m    592\u001b[39m     DEFAULT_SAVE_PEAK_MEMORY_FACTOR \u001b[38;5;28;01mif\u001b[39;00m save_peak_mem \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    593\u001b[39m )\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m    596\u001b[39m     get_autocast_context(device, enabled=autocast),\n\u001b[32m    597\u001b[39m     torch.inference_mode(\u001b[38;5;28mself\u001b[39m.inference_mode),\n\u001b[32m    598\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_full\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m        \u001b[49m\u001b[43monly_return_standard_out\u001b[49m\u001b[43m=\u001b[49m\u001b[43monly_return_standard_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcategorical_inds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatched_cat_ix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    604\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_peak_memory_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_peak_memory_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Git-repository\\TabPFN\\src\\tabpfn\\architectures\\base\\transformer.py:560\u001b[39m, in \u001b[36mPerFeatureTransformer.forward\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    558\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m only_return_standard_out:\n\u001b[32m    559\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.decoder_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     output_decoded = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstandard\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_encoder_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    562\u001b[39m     output_decoded = (\n\u001b[32m    563\u001b[39m         {k: v(test_encoder_out) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.decoder_dict.items()}\n\u001b[32m    564\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.decoder_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    565\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m    566\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\PPML\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"训练Post-hoc集成TabPFN模型\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 检查GPU状态\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch CUDA 可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 设备: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA 设备数量: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    raise RuntimeError(\"CUDA 不可用，无法继续\")\n",
    "\n",
    "# 检查Ray版本\n",
    "import ray\n",
    "\n",
    "print(f\"Ray 版本: {ray.__version__}\")\n",
    "\n",
    "\n",
    "# Post-hoc集成\n",
    "tabpfn_auto = AutoTabPFNClassifier(\n",
    "    max_time=18000,  # 5h\n",
    "    # max_time=12600, # 3.5h\n",
    "    presets=\"best_quality\",\n",
    "    device=\"cuda:0\",\n",
    "    ignore_pretraining_limits=True,  # 允许超过10000样本限制\n",
    ")\n",
    "\n",
    "print(\"模型初始化成功\")\n",
    "print(\"使用设备: cuda:0\")\n",
    "print(f\"训练样本数: {len(X_train_sampled)} (已启用ignore_pretraining_limits)\")\n",
    "print(\"开始自动集成训练...\")\n",
    "print(\"(这可能需要较长时间，正在训练多个模型配置...)\")\n",
    "\n",
    "tabpfn_auto.fit(X_train_sampled, y_train_sampled)\n",
    "\n",
    "y_pred_auto = tabpfn_auto.predict(X_validation_processed)\n",
    "y_pred_proba_auto = tabpfn_auto.predict_proba(X_validation_processed)[:, 1]\n",
    "\n",
    "auc_auto = roc_auc_score(y_validation_binary, y_pred_proba_auto)\n",
    "f1_auto = f1_score(y_validation_binary, y_pred_auto)\n",
    "acc_auto = accuracy_score(y_validation_binary, y_pred_auto)\n",
    "\n",
    "print(f\"\\nPost-hoc集成TabPFN性能:\")\n",
    "print(f\"  AUC: {auc_auto:.4f}\")\n",
    "print(f\"  F1:  {f1_auto:.4f}\")\n",
    "print(f\"  ACC: {acc_auto:.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "joblib.dump(\n",
    "    {\"model\": tabpfn_auto, \"imputer\": imputer},\n",
    "    \"./output/enhanced/models/tabpfn_auto.pkl\",\n",
    ")\n",
    "print(f\"\\n模型已保存: ./output/enhanced/models/tabpfn_auto.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 无监督学习：异常检测（Unsupervised）\n",
    "\n",
    "使用TabPFN进行异常样本检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TabPFN嵌入提取分析\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"提取TabPFN内部嵌入表示...\\n\")\n",
    "\n",
    "# 使用已训练的基础TabPFN模型作为嵌入提取器\n",
    "embedding_extractor = TabPFNEmbedding(tabpfn_clf=tabpfn_basic, n_fold=0)\n",
    "\n",
    "# 提取训练集嵌入\n",
    "print(\"提取训练集嵌入...\")\n",
    "train_embeddings = embedding_extractor.get_embeddings(\n",
    "    X_train_sampled, y_train_sampled, X_train_sampled, data_source=\"train\"\n",
    ")\n",
    "\n",
    "# 提取验证集嵌入\n",
    "print(\"提取验证集嵌入...\")\n",
    "val_embeddings = embedding_extractor.get_embeddings(\n",
    "    X_train_sampled, y_train_sampled, X_validation_processed, data_source=\"test\"\n",
    ")\n",
    "\n",
    "print(f\"嵌入维度: {train_embeddings[0].shape}\")\n",
    "\n",
    "# 使用嵌入训练逻辑回归\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "embedding_model = LogisticRegression(max_iter=1000, random_state=RANDOM_SEED)\n",
    "embedding_model.fit(train_embeddings[0], y_train_sampled)\n",
    "\n",
    "y_pred_embedding = embedding_model.predict(val_embeddings[0])\n",
    "y_pred_proba_embedding = embedding_model.predict_proba(val_embeddings[0])[:, 1]\n",
    "\n",
    "auc_embedding = roc_auc_score(y_validation_binary, y_pred_proba_embedding)\n",
    "f1_embedding = f1_score(y_validation_binary, y_pred_embedding)\n",
    "acc_embedding = accuracy_score(y_validation_binary, y_pred_embedding)\n",
    "\n",
    "print(f\"\\n基于TabPFN嵌入的逻辑回归性能:\")\n",
    "print(f\"  AUC: {auc_embedding:.4f}\")\n",
    "print(f\"  F1:  {f1_embedding:.4f}\")\n",
    "print(f\"  ACC: {acc_embedding:.4f}\")\n",
    "\n",
    "# 可视化嵌入空间（使用PCA降维）\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "val_embeddings_2d = pca.fit_transform(val_embeddings[0])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    val_embeddings_2d[:, 0],\n",
    "    val_embeddings_2d[:, 1],\n",
    "    c=y_validation_binary,\n",
    "    cmap=\"coolwarm\",\n",
    "    alpha=0.6,\n",
    "    edgecolors=\"k\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "plt.colorbar(scatter, label=\"标签 (0=正常, 1=早熟)\")\n",
    "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.1%} 方差)\", fontsize=12)\n",
    "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.1%} 方差)\", fontsize=12)\n",
    "plt.title(\"TabPFN嵌入空间可视化 (PCA降维)\", fontsize=14, fontweight=\"bold\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./output/enhanced/TabPFN嵌入可视化.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(\"./output/enhanced/TabPFN嵌入可视化.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n嵌入空间可视化已保存\")\n",
    "\n",
    "# 保存嵌入\n",
    "np.save(\"./output/enhanced/train_embeddings.npy\", train_embeddings[0])\n",
    "np.save(\"./output/enhanced/val_embeddings.npy\", val_embeddings[0])\n",
    "print(\"嵌入数据已保存\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *导入模型文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"导入已训练的模型并计算性能\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# 模型文件路径\n",
    "model_files = {\n",
    "    \"基础TabPFN\": \"./output/enhanced/models/tabpfn_basic.pkl\",\n",
    "    \"HPO优化TabPFN\": \"./output/enhanced/models/tabpfn_hpo.pkl\",\n",
    "    \"Post-hoc集成TabPFN\": \"./output/enhanced/models/tabpfn_auto.pkl\",\n",
    "}\n",
    "\n",
    "# 导入模型\n",
    "loaded_models = {}\n",
    "for model_name, file_path in model_files.items():\n",
    "    if os.path.exists(file_path):\n",
    "        loaded_models[model_name] = joblib.load(file_path)\n",
    "\n",
    "print(f\"成功导入 {len(loaded_models)} 个模型\")\n",
    "\n",
    "# 计算性能\n",
    "if loaded_models:\n",
    "    # 提取模型\n",
    "    tabpfn_basic = loaded_models[\"基础TabPFN\"][\"model\"]\n",
    "    tabpfn_hpo = loaded_models[\"HPO优化TabPFN\"][\"model\"]\n",
    "    tabpfn_auto = loaded_models[\"Post-hoc集成TabPFN\"][\"model\"]\n",
    "    imputer = loaded_models[\"基础TabPFN\"][\"imputer\"]\n",
    "\n",
    "    # 基础TabPFN\n",
    "    y_pred_basic = tabpfn_basic.predict(X_validation_processed)\n",
    "    y_pred_proba_basic = tabpfn_basic.predict_proba(X_validation_processed)[:, 1]\n",
    "    auc_basic = roc_auc_score(y_validation_binary, y_pred_proba_basic)\n",
    "    f1_basic = f1_score(y_validation_binary, y_pred_basic)\n",
    "    acc_basic = accuracy_score(y_validation_binary, y_pred_basic)\n",
    "\n",
    "    # HPO优化TabPFN\n",
    "    y_pred_hpo = tabpfn_hpo.predict(X_validation_processed)\n",
    "    y_pred_proba_hpo = tabpfn_hpo.predict_proba(X_validation_processed)[:, 1]\n",
    "    auc_hpo = roc_auc_score(y_validation_binary, y_pred_proba_hpo)\n",
    "    f1_hpo = f1_score(y_validation_binary, y_pred_hpo)\n",
    "    acc_hpo = accuracy_score(y_validation_binary, y_pred_hpo)\n",
    "\n",
    "    # Post-hoc集成TabPFN\n",
    "    y_pred_auto = tabpfn_auto.predict(X_validation_processed)\n",
    "    y_pred_proba_auto = tabpfn_auto.predict_proba(X_validation_processed)[:, 1]\n",
    "    auc_auto = roc_auc_score(y_validation_binary, y_pred_proba_auto)\n",
    "    f1_auto = f1_score(y_validation_binary, y_pred_auto)\n",
    "    acc_auto = accuracy_score(y_validation_binary, y_pred_auto)\n",
    "\n",
    "    # 打印性能对比\n",
    "    print(\"\\n模型性能对比：\")\n",
    "    print(\n",
    "        f\"  基础TabPFN:      F1={f1_basic:.4f}  AUC={auc_basic:.4f}  ACC={acc_basic:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  HPO优化:         F1={f1_hpo:.4f}  AUC={auc_hpo:.4f}  ACC={acc_hpo:.4f}  ({f1_hpo-f1_basic:+.4f})\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Post-hoc集成:    F1={f1_auto:.4f}  AUC={auc_auto:.4f}  ACC={acc_auto:.4f}  ({f1_auto-f1_basic:+.4f})\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n性能计算完成，变量已就绪\")\n",
    "else:\n",
    "    print(\"\\n未导入任何模型\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.性能对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建性能对比表\n",
    "models_list = [\n",
    "    (\"基础TabPFN\", auc_basic, f1_basic, acc_basic),\n",
    "    (\"HPO优化TabPFN\", auc_hpo, f1_hpo, acc_hpo),\n",
    "    (\"Post-hoc集成TabPFN\", auc_auto, f1_auto, acc_auto),\n",
    "]\n",
    "\n",
    "performance_summary = pd.DataFrame(models_list, columns=[\"模型\", \"AUC\", \"F1\", \"准确率\"])\n",
    "# 按F1分数排序（从高到低）\n",
    "performance_summary = performance_summary.sort_values(\"F1\", ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TabPFN模型性能对比（以F1分数为准）\")\n",
    "print(\"=\" * 70)\n",
    "print(performance_summary.to_string(index=False))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "best_model_name = performance_summary.iloc[0][\"模型\"]\n",
    "best_f1 = performance_summary.iloc[0][\"F1\"]\n",
    "best_auc = performance_summary.iloc[0][\"AUC\"]\n",
    "f1_improvement = (best_f1 - f1_basic) * 100\n",
    "\n",
    "print(f\"\\n最佳模型: {best_model_name}\")\n",
    "print(f\"   F1:  {best_f1:.4f} (相比基础模型: {f1_improvement:+.2f}%)\")\n",
    "print(f\"   AUC: {best_auc:.4f}\")\n",
    "print(f\"   ACC: {performance_summary.iloc[0]['准确率']:.4f}\")\n",
    "\n",
    "# 保存性能对比\n",
    "performance_summary.to_csv(\n",
    "    \"./output/enhanced/性能对比.csv\", index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(f\"\\n性能对比已保存: ./output/enhanced/性能对比.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. ROC曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9))\n",
    "\n",
    "# 基础TabPFN\n",
    "fpr_basic, tpr_basic, _ = roc_curve(y_validation_binary, y_pred_proba_basic)\n",
    "plt.plot(\n",
    "    fpr_basic,\n",
    "    tpr_basic,\n",
    "    label=f\"基础TabPFN (AUC = {auc_basic:.4f})\",\n",
    "    linewidth=2,\n",
    "    color=\"#2E86AB\",\n",
    "    linestyle=\"-\",\n",
    ")\n",
    "\n",
    "# HPO优化TabPFN\n",
    "fpr_hpo, tpr_hpo, _ = roc_curve(y_validation_binary, y_pred_proba_hpo)\n",
    "plt.plot(\n",
    "    fpr_hpo,\n",
    "    tpr_hpo,\n",
    "    label=f\"HPO优化TabPFN (AUC = {auc_hpo:.4f})\",\n",
    "    linewidth=2,\n",
    "    color=\"#A23B72\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "\n",
    "# Post-hoc集成TabPFN\n",
    "fpr_auto, tpr_auto, _ = roc_curve(y_validation_binary, y_pred_proba_auto)\n",
    "plt.plot(\n",
    "    fpr_auto,\n",
    "    tpr_auto,\n",
    "    label=f\"Post-hoc集成TabPFN (AUC = {auc_auto:.4f})\",\n",
    "    linewidth=2,\n",
    "    color=\"#F18F01\",\n",
    "    linestyle=\"-.\",\n",
    ")\n",
    "\n",
    "# 随机猜测基线\n",
    "plt.plot([0, 1], [0, 1], \"k--\", linewidth=1, label=\"随机猜测\", alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"假阳性率 (1-特异度)\", fontsize=13)\n",
    "plt.ylabel(\"真阳性率 (灵敏度)\", fontsize=13)\n",
    "plt.title(\"TabPFN模型ROC曲线对比\", fontsize=15, fontweight=\"bold\")\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"./output/enhanced/ROC曲线对比.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(\"./output/enhanced/ROC曲线对比.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"ROC曲线对比已保存\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. 详细性能指标对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为所有模型计算详细指标\n",
    "models_results = {\n",
    "    \"基础TabPFN\": (y_pred_basic, y_pred_proba_basic),\n",
    "    \"HPO优化TabPFN\": (y_pred_hpo, y_pred_proba_hpo),\n",
    "    \"Post-hoc集成TabPFN\": (y_pred_auto, y_pred_proba_auto),\n",
    "}\n",
    "\n",
    "detailed_metrics = []\n",
    "\n",
    "for model_name, (y_pred, y_pred_proba) in models_results.items():\n",
    "    cm = confusion_matrix(y_validation_binary, y_pred)\n",
    "\n",
    "    accuracy = accuracy_score(y_validation_binary, y_pred)\n",
    "    precision = precision_score(y_validation_binary, y_pred)\n",
    "    recall = recall_score(y_validation_binary, y_pred)\n",
    "    f1 = f1_score(y_validation_binary, y_pred)\n",
    "    auc = roc_auc_score(y_validation_binary, y_pred_proba)\n",
    "    specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "\n",
    "    detailed_metrics.append(\n",
    "        {\n",
    "            \"模型\": model_name,\n",
    "            \"AUC\": auc,\n",
    "            \"准确率\": accuracy,\n",
    "            \"精确率\": precision,\n",
    "            \"召回率\": recall,\n",
    "            \"特异度\": specificity,\n",
    "            \"F1分数\": f1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{model_name} 详细指标\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"混淆矩阵:\")\n",
    "    print(cm)\n",
    "    print(f\"TN={cm[0,0]}, FP={cm[0,1]}, FN={cm[1,0]}, TP={cm[1,1]}\")\n",
    "    print(f\"\\n准确率: {accuracy:.4f}\")\n",
    "    print(f\"精确率: {precision:.4f}\")\n",
    "    print(f\"召回率: {recall:.4f}\")\n",
    "    print(f\"特异度: {specificity:.4f}\")\n",
    "    print(f\"F1分数: {f1:.4f}\")\n",
    "    print(f\"AUC:    {auc:.4f}\")\n",
    "\n",
    "# 保存详细指标\n",
    "detailed_df = pd.DataFrame(detailed_metrics)\n",
    "detailed_df = detailed_df.sort_values(\"F1分数\", ascending=False)\n",
    "detailed_df.to_csv(\n",
    "    \"./output/enhanced/详细性能指标.csv\", index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(\"所有模型详细指标汇总\")\n",
    "print(f\"{'='*70}\")\n",
    "print(detailed_df.to_string(index=False))\n",
    "print(f\"\\n详细性能指标已保存: ./output/enhanced/详细性能指标.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. 预测概率分布对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "\n",
    "models_plot = [\n",
    "    (\"基础TabPFN\", y_pred_proba_basic),\n",
    "    (\"HPO优化TabPFN\", y_pred_proba_hpo),\n",
    "    (\"Post-hoc集成TabPFN\", y_pred_proba_auto),\n",
    "]\n",
    "\n",
    "for idx, (model_name, y_pred_proba) in enumerate(models_plot):\n",
    "    # 概率分布直方图\n",
    "    ax1 = axes[idx, 0]\n",
    "    for label, name in [(0, \"正常(N)\"), (1, \"早熟(Y)\")]:\n",
    "        mask = y_validation_binary == label\n",
    "        ax1.hist(y_pred_proba[mask], bins=30, alpha=0.6, label=name)\n",
    "    ax1.set_xlabel(\"预测概率\", fontsize=11)\n",
    "    ax1.set_ylabel(\"样本数量\", fontsize=11)\n",
    "    ax1.set_title(f\"{model_name} - 预测概率分布\", fontsize=12, fontweight=\"bold\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "\n",
    "    # 概率箱线图\n",
    "    ax2 = axes[idx, 1]\n",
    "    data_plot = pd.DataFrame(\n",
    "        {\n",
    "            \"概率\": y_pred_proba,\n",
    "            \"真实标签\": [\n",
    "                \"正常(N)\" if y == 0 else \"早熟(Y)\" for y in y_validation_binary\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "    sns.boxplot(data=data_plot, x=\"真实标签\", y=\"概率\", ax=ax2)\n",
    "    ax2.set_title(f\"{model_name} - 预测概率箱线图\", fontsize=12, fontweight=\"bold\")\n",
    "    ax2.grid(alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./output/enhanced/预测概率分布对比.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(\"./output/enhanced/预测概率分布对比.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"预测概率分布对比图已保存\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存所有模型的预测结果\n",
    "predictions_dict = {\n",
    "    \"真实标签\": y_validation_binary,\n",
    "    \"基础TabPFN_预测\": y_pred_basic,\n",
    "    \"基础TabPFN_概率\": y_pred_proba_basic,\n",
    "    \"HPO优化TabPFN_预测\": y_pred_hpo,\n",
    "    \"HPO优化TabPFN_概率\": y_pred_proba_hpo,\n",
    "    \"Post-hoc集成TabPFN_预测\": y_pred_auto,\n",
    "    \"Post-hoc集成TabPFN_概率\": y_pred_proba_auto,\n",
    "}\n",
    "\n",
    "# 添加嵌入模型结果（如果可用）\n",
    "if auc_embedding is not None:\n",
    "    predictions_dict[\"TabPFN嵌入_预测\"] = y_pred_embedding\n",
    "    predictions_dict[\"TabPFN嵌入_概率\"] = y_pred_proba_embedding\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions_dict)\n",
    "\n",
    "predictions_df.to_csv(\n",
    "    \"./output/enhanced/验证集预测结果_完整版.csv\", index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(f\"预测结果已保存: ./output/enhanced/验证集预测结果_完整版.csv\")\n",
    "print(f\"   验证集共 {len(predictions_df)} 样本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存所有模型的预测结果\n",
    "predictions_df = pd.DataFrame(\n",
    "    {\n",
    "        \"真实标签\": y_validation_binary,\n",
    "        \"基础TabPFN_预测\": y_pred_basic,\n",
    "        \"基础TabPFN_概率\": y_pred_proba_basic,\n",
    "        \"HPO优化TabPFN_预测\": y_pred_hpo,\n",
    "        \"HPO优化TabPFN_概率\": y_pred_proba_hpo,\n",
    "        \"Post-hoc集成TabPFN_预测\": y_pred_auto,\n",
    "        \"Post-hoc集成TabPFN_概率\": y_pred_proba_auto,\n",
    "    }\n",
    ")\n",
    "\n",
    "predictions_df.to_csv(\n",
    "    \"./output/enhanced/验证集预测结果.csv\", index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(f\"预测结果已保存: ./output/enhanced/验证集预测结果.csv\")\n",
    "print(f\"   验证集共 {len(predictions_df)} 样本\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. 性能提升总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"性能提升总结\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 计算相对于基础模型的提升\n",
    "improvements = pd.DataFrame(\n",
    "    {\n",
    "        \"模型\": [\"HPO优化TabPFN\", \"Post-hoc集成TabPFN\"],\n",
    "        \"AUC提升(%)\": [(auc_hpo - auc_basic) * 100, (auc_auto - auc_basic) * 100],\n",
    "        \"F1提升(%)\": [(f1_hpo - f1_basic) * 100, (f1_auto - f1_basic) * 100],\n",
    "        \"准确率提升(%)\": [(acc_hpo - acc_basic) * 100, (acc_auto - acc_basic) * 100],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(improvements.to_string(index=False))\n",
    "\n",
    "print(f\"\\n基础模型:\")\n",
    "print(f\"  AUC: {auc_basic:.4f}\")\n",
    "print(f\"  F1:  {f1_basic:.4f}\")\n",
    "print(f\"  ACC: {acc_basic:.4f}\")\n",
    "\n",
    "print(f\"\\n最佳模型: {best_model_name}\")\n",
    "print(f\"  AUC: {best_auc:.4f} ({(best_auc - auc_basic)*100:+.2f}%)\")\n",
    "print(\n",
    "    f\"  F1:  {performance_summary.iloc[0]['F1']:.4f} ({(performance_summary.iloc[0]['F1'] - f1_basic)*100:+.2f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  ACC: {performance_summary.iloc[0]['准确率']:.4f} ({(performance_summary.iloc[0]['准确率'] - acc_basic)*100:+.2f}%)\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SHAP可解释性分析\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 使用HPO优化模型进行SHAP分析\n",
    "best_model = tabpfn_hpo\n",
    "model_name = \"HPO优化TabPFN\"\n",
    "best_f1_score = f1_hpo\n",
    "\n",
    "print(f\"使用模型: {model_name}\")\n",
    "print(f\"F1分数: {best_f1_score:.4f}\")\n",
    "print(f\"计算SHAP值...\\n\")\n",
    "\n",
    "# 选择部分样本计算SHAP值\n",
    "n_samples_shap = min(10, len(X_validation_processed))\n",
    "X_shap = X_validation_processed[:n_samples_shap]\n",
    "\n",
    "# 计算SHAP值\n",
    "shap_values = interpretability.shap.get_shap_values(\n",
    "    estimator=best_model,\n",
    "    test_x=X_shap,\n",
    "    attribute_names=feature_cols_processed,\n",
    "    algorithm=\"permutation\",\n",
    ")\n",
    "\n",
    "# 提取SHAP值数组（正类）\n",
    "shap_array = shap_values.values[:, :, 1]  # (样本数, 特征数)\n",
    "print(f\"SHAP值形状: {shap_array.shape}\\n\")\n",
    "\n",
    "# 计算特征重要性\n",
    "mean_shap = np.abs(shap_array).mean(axis=0)\n",
    "feature_importance = pd.DataFrame(\n",
    "    {\"特征\": feature_cols_processed, \"重要性\": mean_shap}\n",
    ").sort_values(\"重要性\", ascending=False)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"生成SHAP可视化图表\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Beeswarm Plot (Summary Plot) - 显示所有样本的SHAP值分布\n",
    "print(\"\\n1. 生成 Beeswarm Summary Plot...\")\n",
    "import shap\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(\n",
    "    shap_array, X_shap, feature_names=feature_cols_processed, show=False, max_display=20\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./output/enhanced/SHAP_summary_beeswarm.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(\"./output/enhanced/SHAP_summary_beeswarm.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\" 已保存: SHAP_summary_beeswarm.png\")\n",
    "\n",
    "# 2. Bar Plot (Aggregate Feature Importance) - 特征重要性总览\n",
    "print(\"\\n2. 生成 Aggregate Feature Importance...\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(\n",
    "    shap_array,\n",
    "    X_shap,\n",
    "    feature_names=feature_cols_processed,\n",
    "    plot_type=\"bar\",\n",
    "    show=False,\n",
    "    max_display=20,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./output/enhanced/SHAP_bar_importance.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(\"./output/enhanced/SHAP_bar_importance.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\" 已保存: SHAP_bar_importance.png\")\n",
    "\n",
    "# 3. Dependence Plot (最重要特征) - 显示特征值与SHAP值的关系\n",
    "print(\"\\n3. 生成 Dependence Plot（最重要特征）...\")\n",
    "top_feature_idx = feature_importance.index[0]\n",
    "top_feature_name = feature_importance.iloc[0][\"特征\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.dependence_plot(\n",
    "    top_feature_idx,\n",
    "    shap_array,\n",
    "    X_shap,\n",
    "    feature_names=feature_cols_processed,\n",
    "    show=False,\n",
    ")\n",
    "plt.title(f\"SHAP Dependence Plot - {top_feature_name}\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    f\"./output/enhanced/SHAP_dependence_{top_feature_name}.png\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.savefig(\n",
    "    f\"./output/enhanced/SHAP_dependence_{top_feature_name}.pdf\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "print(f\"已保存: SHAP_dependence_{top_feature_name}.png\")\n",
    "\n",
    "# 打印前10个最重要特征\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"前10个最重要特征\")\n",
    "print(\"=\" * 70)\n",
    "print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"SHAP分析完成\")\n",
    "print(\"=\" * 70)\n",
    "print(\"生成的图表：\")\n",
    "print(\"   1. SHAP_summary_beeswarm.png - 特征影响分布图\")\n",
    "print(\"   2. SHAP_bar_importance.png - 特征重要性条形图\")\n",
    "print(f\"   3. SHAP_dependence_{top_feature_name}.png - 最重要特征依赖图\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. PDP部分依赖图分析（新功能）\n",
    "\n",
    "使用tabpfn-extensions 0.2.2新增的partial_dependence_plots功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PDP部分依赖图分析 (tabpfn-extensions 0.2.2新功能)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "from tabpfn_extensions.interpretability import partial_dependence_plots\n",
    "\n",
    "# 选择最重要的特征进行PDP分析\n",
    "top_features = feature_importance.head(6)[\"特征\"].tolist()\n",
    "top_feature_indices = [feature_cols_processed.index(f) for f in top_features]\n",
    "\n",
    "print(f\"分析前6个最重要特征: {top_features}\")\n",
    "\n",
    "# 使用部分验证集样本进行PDP分析\n",
    "n_pdp_samples = min(500, len(X_validation_processed))\n",
    "X_pdp = X_validation_processed[:n_pdp_samples]\n",
    "\n",
    "# 为每个重要特征生成PDP图\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (feature_idx, feature_name) in enumerate(\n",
    "    zip(top_feature_indices, top_features)\n",
    "):\n",
    "    print(f\"  生成 {feature_name} 的PDP图...\")\n",
    "    ax = axes[idx]\n",
    "\n",
    "    try:\n",
    "        disp = partial_dependence_plots(\n",
    "            estimator=tabpfn_hpo,\n",
    "            X=X_pdp,\n",
    "            features=[feature_idx],\n",
    "            kind=\"average\",\n",
    "            target_class=1,  # 正类（早熟）\n",
    "            ax=ax,\n",
    "            grid_resolution=30,\n",
    "        )\n",
    "        ax.set_title(f\"PDP: {feature_name}\", fontsize=11, fontweight=\"bold\")\n",
    "        ax.set_xlabel(feature_name, fontsize=10)\n",
    "        ax.set_ylabel(\"部分依赖\", fontsize=10)\n",
    "        ax.grid(alpha=0.3)\n",
    "    except Exception as e:\n",
    "        ax.text(0.5, 0.5, f\"Error: {str(e)[:30]}...\", ha=\"center\", va=\"center\")\n",
    "        ax.set_title(f\"PDP: {feature_name} (错误)\", fontsize=11)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"部分依赖图 - 特征对预测概率的边际影响\", fontsize=14, fontweight=\"bold\", y=1.02\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./output/enhanced/PDP_top_features.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(\"./output/enhanced/PDP_top_features.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPDP分析完成！\")\n",
    "print(\"已保存: ./output/enhanced/PDP_top_features.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. ShapIQ特征交互分析\n",
    "\n",
    "使用tabpfn-extensions的ShapIQ功能分析特征间的交互效应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ShapIQ特征交互分析 (tabpfn-extensions 0.2.2新功能)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "from tabpfn_extensions.interpretability import get_tabpfn_explainer\n",
    "\n",
    "# 使用部分数据进行ShapIQ分析（计算量较大）\n",
    "n_shapiq_samples = min(100, len(X_train_sampled))\n",
    "n_explain_samples = min(5, len(X_validation_processed))\n",
    "\n",
    "X_background = X_train_sampled[:n_shapiq_samples]\n",
    "y_background = y_train_sampled[:n_shapiq_samples]\n",
    "X_explain = X_validation_processed[:n_explain_samples]\n",
    "\n",
    "print(f\"背景数据: {n_shapiq_samples} 样本\")\n",
    "print(f\"解释样本: {n_explain_samples} 个\")\n",
    "\n",
    "try:\n",
    "    # 创建TabPFN解释器\n",
    "    print(\"\\n创建ShapIQ解释器...\")\n",
    "    explainer = get_tabpfn_explainer(\n",
    "        model=tabpfn_hpo,\n",
    "        data=X_background,\n",
    "        labels=y_background,\n",
    "        index=\"k-SII\",  # k-Shapley交互指数\n",
    "        max_order=2,  # 考虑二阶交互\n",
    "        class_index=1,  # 解释正类（早熟）\n",
    "    )\n",
    "\n",
    "    print(\"计算Shapley交互值...\")\n",
    "    # 解释单个样本\n",
    "    interaction_values = explainer.explain(X_explain[0:1])\n",
    "\n",
    "    print(f\"\\n交互值形状: {interaction_values.values.shape}\")\n",
    "\n",
    "    # 可视化交互效应\n",
    "    print(\"\\n生成交互效应图...\")\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "    # 使用shapiq的可视化\n",
    "    interaction_values.plot_network(\n",
    "        feature_names=feature_cols_processed,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    plt.title(\"ShapIQ特征交互网络图\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        \"./output/enhanced/ShapIQ_interaction_network.png\", dpi=300, bbox_inches=\"tight\"\n",
    "    )\n",
    "    plt.savefig(\n",
    "        \"./output/enhanced/ShapIQ_interaction_network.pdf\", dpi=300, bbox_inches=\"tight\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nShapIQ分析完成！\")\n",
    "    print(\"已保存: ./output/enhanced/ShapIQ_interaction_network.png\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"\\n注意: ShapIQ需要安装shapiq库\")\n",
    "    print(f\"请运行: pip install shapiq\")\n",
    "    print(f\"错误详情: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nShapIQ分析遇到问题: {e}\")\n",
    "    print(\"这可能是由于shapiq库版本兼容性问题，可以跳过此分析\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PPML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
