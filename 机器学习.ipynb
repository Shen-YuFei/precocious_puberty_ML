{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 性早熟预测模型\n",
    "\n",
    "**基线数据 + 动态特征结合**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 3080 Laptop GPU\n",
      "显存: 16.0 GB\n",
      "使用MissForest方法进行缺失值填补\n"
     ]
    }
   ],
   "source": [
    "# 设置环境变量（必须在导入sklearn之前）\n",
    "import os\n",
    "\n",
    "os.environ[\"SCIPY_ARRAY_API\"] = \"1\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.metrics import roc_curve\n",
    "import xgboost as xgb\n",
    "from tabpfn import TabPFNClassifier\n",
    "import tabm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n",
    "\n",
    "\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"SimHei\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "import re\n",
    "import joblib\n",
    "\n",
    "\n",
    "# 定义MissForest插补器工厂函数\n",
    "def create_missforest_imputer(random_state=825):\n",
    "    \"\"\"创建MissForest插补器（IterativeImputer + RandomForest）\"\"\"\n",
    "    return IterativeImputer(\n",
    "        estimator=RandomForestRegressor(\n",
    "            n_estimators=10,\n",
    "            max_depth=10,\n",
    "            n_jobs=-1,\n",
    "            random_state=random_state,\n",
    "        ),\n",
    "        max_iter=10,\n",
    "        random_state=random_state,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"显存: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(\"使用MissForest方法进行缺失值填补\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 设置路径和参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./output\", exist_ok=True)\n",
    "os.makedirs(\"./output/models\", exist_ok=True)\n",
    "\n",
    "RANDOM_SEED = 825\n",
    "np.random.seed(RANDOM_SEED)\n",
    "N_JOBS = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正常组: 8970 行, 早熟组: 10654 行\n"
     ]
    }
   ],
   "source": [
    "normal_data = pd.read_csv(\"./input/性早熟数据激发试验正常组_new.csv\")\n",
    "disease_data = pd.read_csv(\"./input/激发试验确诊性早熟组数据_new.csv\")\n",
    "\n",
    "normal_data[\"group\"] = \"N\"\n",
    "disease_data[\"group\"] = \"Y\"\n",
    "\n",
    "print(f\"正常组: {normal_data.shape[0]} 行, 早熟组: {disease_data.shape[0]} 行\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 数据类型处理和合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并后数据: 19624 行 x 40 列\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat([normal_data, disease_data], axis=0, ignore_index=True)\n",
    "data[\"group\"] = data[\"group\"].astype(\"category\")\n",
    "print(f\"合并后数据: {data.shape[0]} 行 x {data.shape[1]} 列\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 查看数据基本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据基本信息:\n",
      "数据维度: (19624, 40)\n",
      "分组统计:\n",
      "group\n",
      "Y    10654\n",
      "N     8970\n",
      "Name: count, dtype: int64\n",
      "数据类型:\n",
      "float64     39\n",
      "category     1\n",
      "Name: count, dtype: int64\n",
      "缺失值统计:\n",
      "总缺失值数量: 335074\n",
      "各列缺失值:\n",
      "有无腋毛           17161\n",
      "右乳腺体宽（cm）      14207\n",
      "左乳腺体宽（cm）      14201\n",
      "乳腺体宽平均值（cm）    14190\n",
      "最大卵泡直径直径       14105\n",
      "有无阴毛           13199\n",
      "左乳腺体厚（cm）      13182\n",
      "右乳腺体厚（cm）      13152\n",
      "乳腺体厚平均值（cm）    13135\n",
      "乳晕色素沉着         12758\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"数据基本信息:\")\n",
    "print(f\"数据维度: {data.shape}\")\n",
    "print(f\"分组统计:\")\n",
    "print(data[\"group\"].value_counts())\n",
    "print(f\"数据类型:\")\n",
    "print(data.dtypes.value_counts())\n",
    "print(f\"缺失值统计:\")\n",
    "missing_count = data.isnull().sum().sum()\n",
    "print(f\"总缺失值数量: {missing_count}\")\n",
    "if missing_count > 0:\n",
    "    missing_by_col = data.isnull().sum()\n",
    "    missing_by_col = missing_by_col[missing_by_col > 0].sort_values(ascending=False)\n",
    "    print(\"各列缺失值:\")\n",
    "    print(missing_by_col.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 划分训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集: 13736 行, 验证集: 5888 行\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data = train_test_split(\n",
    "    data, test_size=0.3, stratify=data[\"group\"], random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"训练集: {train_data.shape[0]} 行, 验证集: {validation_data.shape[0]} 行\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用 38 个特征\n"
     ]
    }
   ],
   "source": [
    "exclude_cols = [\"group\", \"患者编号\", \"Unnamed: 0\"]\n",
    "feature_cols = [col for col in train_data.columns if col not in exclude_cols]\n",
    "\n",
    "X_train = train_data[feature_cols].copy()\n",
    "y_train = train_data[\"group\"].copy()\n",
    "X_validation = validation_data[feature_cols].copy()\n",
    "y_validation = validation_data[\"group\"].copy()\n",
    "\n",
    "y_train_binary = (y_train == \"Y\").astype(int)\n",
    "y_validation_binary = (y_validation == \"Y\").astype(int)\n",
    "\n",
    "print(f\"使用 {len(feature_cols)} 个特征\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 特征概览和缺失值分析/补缺（MissForest方法）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征列表 (38个特征):\n",
      "   1. 年龄\n",
      "   2. 身高（cm）\n",
      "   3. 体重（kg）\n",
      "   4. BMI （体重Kg÷身高m）\n",
      "   5. Tanner分期\n",
      "   6. 乳晕色素沉着\n",
      "   7. 乳核\n",
      "   8. 有无阴毛\n",
      "   9. 有无腋毛\n",
      "  10. 基础血清促黄体生成激素（LH）\n",
      "  11. 基础血清卵泡刺激素（FSH）\n",
      "  12. 子宫长（cm）\n",
      "  13. 子宫宽（cm）\n",
      "  14. 子宫厚（cm）\n",
      "  15. 子宫体积（长X宽X厚X0.5236）\n",
      "  16. 最大卵泡直径直径\n",
      "  17. 左卵巢长（cm）\n",
      "  18. 左卵巢宽（cm）\n",
      "  19. 左卵巢厚（cm）\n",
      "  20. 左卵巢体积（长X宽X厚X0.5233）\n",
      "  21. 右卵巢长（cm）\n",
      "  22. 右卵巢宽（cm）\n",
      "  23. 右卵巢厚（cm）\n",
      "  24. 右卵巢体积（长X宽X厚X0.5233）\n",
      "  25. 卵巢长平均值（cm）\n",
      "  26. 卵巢宽平均值（cm）\n",
      "  27. 卵巢厚平均值（cm）\n",
      "  28. 卵巢体积平均值\n",
      "  29. 左乳腺体宽（cm）\n",
      "  30. 左乳腺体厚（cm）\n",
      "  31. 右乳腺体宽（cm）\n",
      "  32. 右乳腺体厚（cm）\n",
      "  33. 乳腺体宽平均值（cm）\n",
      "  34. 乳腺体厚平均值（cm）\n",
      "  35. 骨龄(岁)\n",
      "  36. 骨龄与实际年龄比值\n",
      "  37. 生物年龄和骨龄之间的差异\n",
      "  38. 按CHN法测算，左手、腕骨发育成熟度评分\n",
      "\n",
      "训练集: 13736样本, 验证集: 5888样本\n",
      "训练集正负样本: {1: 7457, 0: 6279}\n",
      "验证集正负样本: {1: 3197, 0: 2691}\n",
      "\n",
      "缺失值分析 (Top 10):\n",
      "              缺失数量  缺失率(%)\n",
      "有无腋毛         11974   87.17\n",
      "右乳腺体宽（cm）     9917   72.20\n",
      "左乳腺体宽（cm）     9913   72.17\n",
      "乳腺体宽平均值（cm）   9905   72.11\n",
      "最大卵泡直径直径      9884   71.96\n",
      "左乳腺体厚（cm）     9215   67.09\n",
      "有无阴毛          9216   67.09\n",
      "右乳腺体厚（cm）     9202   66.99\n",
      "乳腺体厚平均值（cm）   9189   66.90\n",
      "乳晕色素沉着        8936   65.06\n",
      "\n",
      "============================================================\n",
      "缺失值填补: MissForest (IterativeImputer + RandomForest)\n",
      "============================================================\n",
      "正在填补缺失值...（可能需要几分钟）\n",
      "\n",
      "填补完成！\n",
      "  X_train_imputed:  (13736, 38) (原始填补数据)\n",
      "  X_train_scaled:   (13736, 38) (标准化数据)\n",
      "  缺失值检查: 0 (应为0)\n"
     ]
    }
   ],
   "source": [
    "# ===== 8. 特征概览 =====\n",
    "print(f\"特征列表 ({len(feature_cols)}个特征):\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\n训练集: {X_train.shape[0]}样本, 验证集: {X_validation.shape[0]}样本\")\n",
    "print(f\"训练集正负样本: {y_train_binary.value_counts().to_dict()}\")\n",
    "print(f\"验证集正负样本: {y_validation_binary.value_counts().to_dict()}\")\n",
    "\n",
    "# ===== 缺失值分析 =====\n",
    "missing_train = X_train.isnull().sum()\n",
    "missing_pct = (missing_train / len(X_train) * 100).round(2)\n",
    "missing_info = (\n",
    "    pd.DataFrame({\"缺失数量\": missing_train, \"缺失率(%)\": missing_pct})\n",
    "    .query(\"缺失数量 > 0\")\n",
    "    .sort_values(\"缺失率(%)\", ascending=False)\n",
    ")\n",
    "\n",
    "if not missing_info.empty:\n",
    "    print(f\"\\n缺失值分析 (Top 10):\")\n",
    "    print(missing_info.head(10).to_string())\n",
    "\n",
    "# ===== 统一缺失值填补（MissForest方法）=====\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"缺失值填补: MissForest (IterativeImputer + RandomForest)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "imputer = create_missforest_imputer(RANDOM_SEED)\n",
    "print(\"正在填补缺失值...（可能需要几分钟）\")\n",
    "\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_validation_imputed = imputer.transform(X_validation)\n",
    "\n",
    "# 转换为DataFrame保持列名\n",
    "X_train_imputed = pd.DataFrame(\n",
    "    X_train_imputed, columns=feature_cols, index=X_train.index\n",
    ")\n",
    "X_validation_imputed = pd.DataFrame(\n",
    "    X_validation_imputed, columns=feature_cols, index=X_validation.index\n",
    ")\n",
    "\n",
    "# 标准化（用于KNN/SVM/NNET等模型）\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_validation_scaled = scaler.transform(X_validation_imputed)\n",
    "\n",
    "print(f\"\\n填补完成！\")\n",
    "print(f\"  缺失值检查: {X_train_imputed.isnull().sum().sum()} (应为0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"数据类型转换:\")\n",
    "print(f\"转换前 - 训练集: {X_train.dtypes.value_counts().to_dict()}\")\n",
    "\n",
    "for col in feature_cols:\n",
    "    X_train[col] = pd.to_numeric(X_train[col], errors=\"coerce\")\n",
    "    X_validation[col] = pd.to_numeric(X_validation[col], errors=\"coerce\")\n",
    "\n",
    "print(f\"转换后 - 训练集: {X_train.dtypes.value_counts().to_dict()}\")\n",
    "\n",
    "missing_after_convert = X_train.isnull().sum().sum()\n",
    "if missing_after_convert > 0:\n",
    "    print(f\"数据类型转换后产生了 {missing_after_convert} 个新的缺失值\")\n",
    "    new_missing = X_train.isnull().sum()\n",
    "    new_missing = new_missing[new_missing > 0].sort_values(ascending=False)\n",
    "    print(\"新增缺失值的列:\")\n",
    "    print(new_missing.to_string())\n",
    "else:\n",
    "    print(\"数据类型转换完成，无新增缺失值\")\n",
    "\n",
    "print(\n",
    "    f\"数据准备完成: {len(feature_cols)}个特征, 训练样本{X_train.shape[0]}, 验证样本{X_validation.shape[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 模型训练\n",
    "\n",
    "## 10. 设置交叉验证策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证策略: 5折交叉验证 x 3次重复 = 15轮\n",
      "训练集: 13736样本, 正负比={1: 7457, 0: 6279}\n",
      "验证集: 5888样本, 正负比={1: 3197, 0: 2691}\n"
     ]
    }
   ],
   "source": [
    "cv_strategy = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=RANDOM_SEED)\n",
    "\n",
    "print(f\"验证策略: 5折交叉验证 x 3次重复 = {cv_strategy.get_n_splits()}轮\")\n",
    "print(f\"训练集: {len(X_train)}样本, 正负比={y_train_binary.value_counts().to_dict()}\")\n",
    "print(\n",
    "    f\"验证集: {len(X_validation)}样本, 正负比={y_validation_binary.value_counts().to_dict()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 训练模型1: GBM (梯度提升机)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM AUC: 0.9414, F1: 0.8741\n"
     ]
    }
   ],
   "source": [
    "gbm_model = HistGradientBoostingClassifier(\n",
    "    max_iter=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.01,\n",
    "    l2_regularization=0.1,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "gbm_model.fit(X_train, y_train_binary)\n",
    "\n",
    "y_pred_gbm = gbm_model.predict(X_validation)\n",
    "y_pred_proba_gbm = gbm_model.predict_proba(X_validation)[:, 1]\n",
    "auc_gbm = roc_auc_score(y_validation_binary, y_pred_proba_gbm)\n",
    "f1_gbm = f1_score(y_validation_binary, y_pred_gbm)\n",
    "\n",
    "joblib.dump(gbm_model, \"./output/models/gbm_model.pkl\")\n",
    "print(f\"GBM AUC: {auc_gbm:.4f}, F1: {f1_gbm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 训练模型2: KNN (K近邻)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN AUC: 0.9048, F1: 0.8538\n"
     ]
    }
   ],
   "source": [
    "# KNN使用标准化数据\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5, n_jobs=N_JOBS)\n",
    "knn_model.fit(X_train_scaled, y_train_binary)\n",
    "\n",
    "y_pred_knn = knn_model.predict(X_validation_scaled)\n",
    "y_pred_proba_knn = knn_model.predict_proba(X_validation_scaled)[:, 1]\n",
    "auc_knn = roc_auc_score(y_validation_binary, y_pred_proba_knn)\n",
    "f1_knn = f1_score(y_validation_binary, y_pred_knn)\n",
    "\n",
    "joblib.dump(\n",
    "    {\"model\": knn_model, \"imputer\": imputer, \"scaler\": scaler},\n",
    "    \"./output/models/knn_model.pkl\",\n",
    ")\n",
    "print(f\"KNN AUC: {auc_knn:.4f}, F1: {f1_knn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 训练模型3: Naive Bayes (朴素贝叶斯)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB AUC: 0.8234, F1: 0.8008\n"
     ]
    }
   ],
   "source": [
    "# NB使用填补后数据（无需标准化）\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train_imputed, y_train_binary)\n",
    "\n",
    "y_pred_nb = nb_model.predict(X_validation_imputed)\n",
    "y_pred_proba_nb = nb_model.predict_proba(X_validation_imputed)[:, 1]\n",
    "auc_nb = roc_auc_score(y_validation_binary, y_pred_proba_nb)\n",
    "f1_nb = f1_score(y_validation_binary, y_pred_nb)\n",
    "\n",
    "joblib.dump({\"model\": nb_model, \"imputer\": imputer}, \"./output/models/nb_model.pkl\")\n",
    "print(f\"NB AUC: {auc_nb:.4f}, F1: {f1_nb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. 训练模型4: XGBoost (极限梯度提升)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB AUC: 0.9644, F1: 0.9000\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.013,\n",
    "    random_state=RANDOM_SEED,\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=N_JOBS,\n",
    ")\n",
    "xgb_model.fit(X_train, y_train_binary)\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_validation)\n",
    "y_pred_proba_xgb = xgb_model.predict_proba(X_validation)[:, 1]\n",
    "auc_xgb = roc_auc_score(y_validation_binary, y_pred_proba_xgb)\n",
    "f1_xgb = f1_score(y_validation_binary, y_pred_xgb)\n",
    "\n",
    "joblib.dump(xgb_model, \"./output/models/xgb_model.pkl\")\n",
    "print(f\"XGB AUC: {auc_xgb:.4f}, F1: {f1_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. 训练模型5: Random Forest (随机森林)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF AUC: 0.9524, F1: 0.8961\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=500, random_state=RANDOM_SEED, n_jobs=N_JOBS\n",
    ")\n",
    "rf_model.fit(X_train, y_train_binary)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_validation)\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_validation)[:, 1]\n",
    "auc_rf = roc_auc_score(y_validation_binary, y_pred_proba_rf)\n",
    "f1_rf = f1_score(y_validation_binary, y_pred_rf)\n",
    "\n",
    "joblib.dump(rf_model, \"./output/models/rf_model.pkl\")\n",
    "print(f\"RF AUC: {auc_rf:.4f}, F1: {f1_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. 训练模型6: RPART (决策树)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPART AUC: 0.9316, F1: 0.8658\n"
     ]
    }
   ],
   "source": [
    "# RPART使用填补后数据\n",
    "rpart_model = DecisionTreeClassifier(\n",
    "    max_depth=5, min_samples_split=50, min_samples_leaf=20, random_state=RANDOM_SEED\n",
    ")\n",
    "rpart_model.fit(X_train_imputed, y_train_binary)\n",
    "\n",
    "y_pred_rpart = rpart_model.predict(X_validation_imputed)\n",
    "y_pred_proba_rpart = rpart_model.predict_proba(X_validation_imputed)[:, 1]\n",
    "auc_rpart = roc_auc_score(y_validation_binary, y_pred_proba_rpart)\n",
    "f1_rpart = f1_score(y_validation_binary, y_pred_rpart)\n",
    "\n",
    "joblib.dump(\n",
    "    {\"model\": rpart_model, \"imputer\": imputer}, \"./output/models/rpart_model.pkl\"\n",
    ")\n",
    "print(f\"RPART AUC: {auc_rpart:.4f}, F1: {f1_rpart:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. 训练模型7: GLM (逻辑回归)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLM AUC: 0.8668, F1: 0.8443\n"
     ]
    }
   ],
   "source": [
    "# GLM使用填补后数据\n",
    "glm_model = LogisticRegression(max_iter=1000, random_state=RANDOM_SEED, n_jobs=N_JOBS)\n",
    "glm_model.fit(X_train_imputed, y_train_binary)\n",
    "\n",
    "y_pred_glm = glm_model.predict(X_validation_imputed)\n",
    "y_pred_proba_glm = glm_model.predict_proba(X_validation_imputed)[:, 1]\n",
    "auc_glm = roc_auc_score(y_validation_binary, y_pred_proba_glm)\n",
    "f1_glm = f1_score(y_validation_binary, y_pred_glm)\n",
    "\n",
    "joblib.dump({\"model\": glm_model, \"imputer\": imputer}, \"./output/models/glm_model.pkl\")\n",
    "print(f\"GLM AUC: {auc_glm:.4f}, F1: {f1_glm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. 训练模型8: SVM (支持向量机)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM使用标准化数据\n",
    "svm_model = SVC(kernel=\"rbf\", probability=True, random_state=RANDOM_SEED)\n",
    "svm_model.fit(X_train_scaled, y_train_binary)\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_validation_scaled)\n",
    "y_pred_proba_svm = svm_model.predict_proba(X_validation_scaled)[:, 1]\n",
    "auc_svm = roc_auc_score(y_validation_binary, y_pred_proba_svm)\n",
    "f1_svm = f1_score(y_validation_binary, y_pred_svm)\n",
    "\n",
    "joblib.dump(\n",
    "    {\"model\": svm_model, \"imputer\": imputer, \"scaler\": scaler},\n",
    "    \"./output/models/svm_model.pkl\",\n",
    ")\n",
    "print(f\"SVM AUC: {auc_svm:.4f}, F1: {f1_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. 训练模型9: NNET (神经网络)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NNET使用标准化数据\n",
    "nnet_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    max_iter=500,\n",
    "    random_state=RANDOM_SEED,\n",
    "    early_stopping=True,\n",
    ")\n",
    "nnet_model.fit(X_train_scaled, y_train_binary)\n",
    "\n",
    "y_pred_nnet = nnet_model.predict(X_validation_scaled)\n",
    "y_pred_proba_nnet = nnet_model.predict_proba(X_validation_scaled)[:, 1]\n",
    "auc_nnet = roc_auc_score(y_validation_binary, y_pred_proba_nnet)\n",
    "f1_nnet = f1_score(y_validation_binary, y_pred_nnet)\n",
    "\n",
    "joblib.dump(\n",
    "    {\"model\": nnet_model, \"imputer\": imputer, \"scaler\": scaler},\n",
    "    \"./output/models/nnet_model.pkl\",\n",
    ")\n",
    "print(f\"NNET AUC: {auc_nnet:.4f}, F1: {f1_nnet:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. 训练模型11: TabM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练TabM模型\n",
    "print(\"=\" * 70)\n",
    "print(\"训练TabM模型\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import rtdl_num_embeddings\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# 使用已填补的数据进行QuantileTransformer\n",
    "X_train_processed = X_train_imputed.values\n",
    "noise = (\n",
    "    np.random.default_rng(0)\n",
    "    .normal(0.0, 1e-5, X_train_processed.shape)\n",
    "    .astype(X_train_processed.dtype)\n",
    ")\n",
    "quantile_transformer = QuantileTransformer(\n",
    "    n_quantiles=max(min(len(X_train_processed) // 30, 1000), 10),\n",
    "    output_distribution=\"normal\",\n",
    "    subsample=10**9,\n",
    ").fit(X_train_processed + noise)\n",
    "\n",
    "X_train_transformed = quantile_transformer.transform(X_train_processed)\n",
    "X_val_transformed = quantile_transformer.transform(X_validation_imputed.values)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_transformed, dtype=torch.float32).cuda()\n",
    "y_train_tensor = torch.tensor(y_train_binary.values, dtype=torch.long).cuda()\n",
    "X_val_tensor = torch.tensor(X_val_transformed, dtype=torch.float32).cuda()\n",
    "\n",
    "print(f\"训练集: {X_train_tensor.shape}, 验证集: {X_val_tensor.shape}\")\n",
    "\n",
    "# PiecewiseLinear嵌入\n",
    "num_embeddings = rtdl_num_embeddings.PiecewiseLinearEmbeddings(\n",
    "    rtdl_num_embeddings.compute_bins(X_train_tensor, n_bins=48),\n",
    "    d_embedding=16,\n",
    "    activation=False,\n",
    "    version=\"B\",\n",
    ")\n",
    "\n",
    "# 创建TabM模型\n",
    "tabm_model = tabm.TabM.make(\n",
    "    n_num_features=X_train_tensor.shape[1],\n",
    "    cat_cardinalities=[],\n",
    "    d_out=2,\n",
    "    num_embeddings=num_embeddings,\n",
    ").cuda()\n",
    "\n",
    "print(f\"模型参数量: {sum(p.numel() for p in tabm_model.parameters()):,}\")\n",
    "\n",
    "# AdamW优化器配置\n",
    "optimizer = torch.optim.AdamW(tabm_model.parameters(), lr=2e-3, weight_decay=3e-4)\n",
    "\n",
    "gradient_clipping_norm = 1.0\n",
    "\n",
    "amp_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "amp_enabled = True\n",
    "print(f\"AMP: {amp_dtype}\\n\")\n",
    "\n",
    "# 训练配置\n",
    "batch_size = 256\n",
    "n_epochs = 500\n",
    "\n",
    "# 训练循环\n",
    "print(\"开始训练\\n\")\n",
    "best_f1 = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    tabm_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    batches = torch.randperm(len(X_train_tensor), device=\"cuda\").split(batch_size)\n",
    "\n",
    "    for batch_idx in batches:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.autocast(device_type=\"cuda\", enabled=amp_enabled, dtype=amp_dtype):\n",
    "            logits = tabm_model(X_train_tensor[batch_idx], None)\n",
    "            y_pred = logits.flatten(0, 1)\n",
    "            y_true = y_train_tensor[batch_idx].repeat_interleave(tabm_model.backbone.k)\n",
    "            loss = nn.functional.cross_entropy(y_pred, y_true)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if gradient_clipping_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                tabm_model.parameters(), gradient_clipping_norm\n",
    "            )\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # 验证\n",
    "    tabm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_logits = tabm_model(X_val_tensor, None)\n",
    "        val_proba = torch.softmax(val_logits, dim=-1).mean(dim=1)[:, 1].cpu().numpy()\n",
    "        val_pred = (val_proba >= 0.5).astype(int)\n",
    "        auc = roc_auc_score(y_validation_binary, val_proba)\n",
    "        f1 = f1_score(y_validation_binary, val_pred)\n",
    "\n",
    "    improved = f1 > best_f1\n",
    "    print(\n",
    "        f\"Epoch {epoch:3d}: Loss={total_loss/len(batches):.4f}, F1={f1:.4f}, AUC={auc:.4f}{' *' if improved else ''}\"\n",
    "    )\n",
    "\n",
    "    if improved:\n",
    "        best_f1, best_auc, best_epoch = f1, auc, epoch\n",
    "        torch.save(tabm_model.state_dict(), \"./output/models/tabm_model.pt\")\n",
    "        joblib.dump(\n",
    "            {\"imputer\": imputer, \"quantile_transformer\": quantile_transformer},\n",
    "            \"./output/models/tabm_preprocessors.pkl\",\n",
    "        )\n",
    "\n",
    "print(f\"\\n训练完成: 最佳F1={best_f1:.4f} (Epoch {best_epoch}), AUC={best_auc:.4f}\")\n",
    "\n",
    "# 最终预测\n",
    "tabm_model.load_state_dict(\n",
    "    torch.load(\"./output/models/tabm_model.pt\", weights_only=True)\n",
    ")\n",
    "tabm_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_proba_tabm = (\n",
    "        torch.softmax(tabm_model(X_val_tensor, None), dim=-1)\n",
    "        .mean(dim=1)[:, 1]\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "    y_pred_tabm = (y_pred_proba_tabm >= 0.5).astype(int)\n",
    "\n",
    "auc_tabm = roc_auc_score(y_validation_binary, y_pred_proba_tabm)\n",
    "f1_tabm = f1_score(y_validation_binary, y_pred_tabm)\n",
    "\n",
    "print(f\"显存峰值: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"\\nTabM最终性能: F1={f1_tabm:.4f}, AUC={auc_tabm:.4f}\")\n",
    "print(f\"模型已保存: ./output/models/tabm_model.pt\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. 加载模型10: TabPFN (Post-hoc集成)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"加载Post-hoc集成TabPFN模型\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 加载模型文件\n",
    "auto_model_path = \"./output/enhanced/models/tabpfn_auto.pkl\"\n",
    "\n",
    "# 加载模型和预处理器\n",
    "auto_data = joblib.load(auto_model_path)\n",
    "tabpfn_model = auto_data[\"model\"]\n",
    "imputer_auto = auto_data[\"imputer\"]\n",
    "\n",
    "print(f\"已加载Post-hoc集成TabPFN模型\")\n",
    "print(f\"模型路径: {auto_model_path}\")\n",
    "print(f\"模型类型: {type(tabpfn_model).__name__}\")\n",
    "\n",
    "# 数据预处理\n",
    "X_train_tabpfn = imputer_auto.transform(X_train)\n",
    "X_validation_tabpfn = imputer_auto.transform(X_validation)\n",
    "\n",
    "# 验证集预测\n",
    "print(f\"\\n在验证集上进行预测...\")\n",
    "y_pred_tabpfn = tabpfn_model.predict(X_validation_tabpfn)\n",
    "y_pred_proba_tabpfn = tabpfn_model.predict_proba(X_validation_tabpfn)[:, 1]\n",
    "\n",
    "# 计算性能指标\n",
    "auc_tabpfn = roc_auc_score(y_validation_binary, y_pred_proba_tabpfn)\n",
    "f1_tabpfn = f1_score(y_validation_binary, y_pred_tabpfn)\n",
    "\n",
    "print(f\"\\nPost-hoc集成TabPFN性能指标:\")\n",
    "print(f\"  AUC: {auc_tabpfn:.4f}\")\n",
    "print(f\"  F1:  {f1_tabpfn:.4f}\")\n",
    "\n",
    "# 保存到与其他模型一致的位置\n",
    "joblib.dump(\n",
    "    {\"model\": tabpfn_model, \"imputer\": imputer_auto},\n",
    "    \"./output/models/tabpfn_model.pkl\",\n",
    ")\n",
    "print(f\"\\n模型已复制到: ./output/models/tabpfn_model.pkl\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型性能汇总\n",
    "models_summary = pd.DataFrame(\n",
    "    {\n",
    "        \"模型\": [\n",
    "            \"GBM\",\n",
    "            \"KNN\",\n",
    "            \"NB\",\n",
    "            \"XGB\",\n",
    "            \"RF\",\n",
    "            \"RPART\",\n",
    "            \"GLM\",\n",
    "            \"SVM\",\n",
    "            \"NNET\",\n",
    "            \"TabPFN-HPO\",\n",
    "            \"TabM\",\n",
    "        ],\n",
    "        \"AUC\": [\n",
    "            auc_gbm,\n",
    "            auc_knn,\n",
    "            auc_nb,\n",
    "            auc_xgb,\n",
    "            auc_rf,\n",
    "            auc_rpart,\n",
    "            auc_glm,\n",
    "            auc_svm,\n",
    "            auc_nnet,\n",
    "            auc_tabpfn,\n",
    "            auc_tabm,\n",
    "        ],\n",
    "        \"F1\": [\n",
    "            f1_gbm,\n",
    "            f1_knn,\n",
    "            f1_nb,\n",
    "            f1_xgb,\n",
    "            f1_rf,\n",
    "            f1_rpart,\n",
    "            f1_glm,\n",
    "            f1_svm,\n",
    "            f1_nnet,\n",
    "            f1_tabpfn,\n",
    "            f1_tabm,\n",
    "        ],\n",
    "    }\n",
    ").sort_values(\"AUC\", ascending=False)\n",
    "\n",
    "best_model_name = models_summary.iloc[0][\"模型\"]\n",
    "best_model_auc = models_summary.iloc[0][\"AUC\"]\n",
    "best_model_f1 = models_summary.iloc[0][\"F1\"]\n",
    "\n",
    "print(f\"模型性能 ({len(models_summary)}个模型):\")\n",
    "print(models_summary.to_string(index=False))\n",
    "print(\n",
    "    f\"\\n最佳模型: {best_model_name} (AUC = {best_model_auc:.4f}, F1 = {best_model_f1:.4f})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# ROC曲线对比\n",
    "models_info = [\n",
    "    (\"GBM\", y_pred_proba_gbm, auc_gbm),\n",
    "    (\"KNN\", y_pred_proba_knn, auc_knn),\n",
    "    (\"NB\", y_pred_proba_nb, auc_nb),\n",
    "    (\"XGB\", y_pred_proba_xgb, auc_xgb),\n",
    "    (\"RF\", y_pred_proba_rf, auc_rf),\n",
    "    (\"RPART\", y_pred_proba_rpart, auc_rpart),\n",
    "    (\"GLM\", y_pred_proba_glm, auc_glm),\n",
    "    (\"SVM\", y_pred_proba_svm, auc_svm),\n",
    "    (\"NNET\", y_pred_proba_nnet, auc_nnet),\n",
    "    (\"TabPFN-HPO\", y_pred_proba_tabpfn, auc_tabpfn),\n",
    "    (\"TabM\", y_pred_proba_tabm, auc_tabm),\n",
    "]\n",
    "\n",
    "for model_name, y_proba, auc_score in models_info:\n",
    "    fpr, tpr, _ = roc_curve(y_validation_binary, y_proba)\n",
    "    linewidth = 3 if model_name in [\"TabPFN-HPO\", \"TabM\"] else 2\n",
    "    plt.plot(\n",
    "        fpr, tpr, label=f\"{model_name} (AUC = {auc_score:.4f})\", linewidth=linewidth\n",
    "    )\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", linewidth=1, label=\"随机猜测\")\n",
    "plt.xlabel(\"假阳性率 (1-特异度)\", fontsize=12)\n",
    "plt.ylabel(\"真阳性率 (灵敏度)\", fontsize=12)\n",
    "plt.title(\"11个模型的ROC曲线对比（含TabPFN-HPO和TabM）\", fontsize=14, fontweight=\"bold\")\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"./output/ROC曲线对比_11模型.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(\"./output/ROC曲线对比_11模型.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"ROC曲线已保存\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存预测结果和性能指标\n",
    "predictions_df = pd.DataFrame(\n",
    "    {\n",
    "        \"真实标签\": y_validation_binary,\n",
    "        \"GBM_预测\": y_pred_gbm,\n",
    "        \"GBM_概率\": y_pred_proba_gbm,\n",
    "        \"KNN_预测\": y_pred_knn,\n",
    "        \"KNN_概率\": y_pred_proba_knn,\n",
    "        \"NB_预测\": y_pred_nb,\n",
    "        \"NB_概率\": y_pred_proba_nb,\n",
    "        \"XGB_预测\": y_pred_xgb,\n",
    "        \"XGB_概率\": y_pred_proba_xgb,\n",
    "        \"RF_预测\": y_pred_rf,\n",
    "        \"RF_概率\": y_pred_proba_rf,\n",
    "        \"RPART_预测\": y_pred_rpart,\n",
    "        \"RPART_概率\": y_pred_proba_rpart,\n",
    "        \"GLM_预测\": y_pred_glm,\n",
    "        \"GLM_概率\": y_pred_proba_glm,\n",
    "        \"SVM_预测\": y_pred_svm,\n",
    "        \"SVM_概率\": y_pred_proba_svm,\n",
    "        \"NNET_预测\": y_pred_nnet,\n",
    "        \"NNET_概率\": y_pred_proba_nnet,\n",
    "        \"TabPFN-HPO_预测\": y_pred_tabpfn,\n",
    "        \"TabPFN-HPO_概率\": y_pred_proba_tabpfn,\n",
    "        \"TabM_预测\": y_pred_tabm,\n",
    "        \"TabM_概率\": y_pred_proba_tabm,\n",
    "    }\n",
    ")\n",
    "\n",
    "predictions_df.to_csv(\n",
    "    \"./output/验证集预测结果_11模型.csv\", index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(f\"预测结果已保存，验证集共 {len(predictions_df)} 样本，包含11个模型的预测\")\n",
    "\n",
    "# 保存模型性能汇总\n",
    "models_summary.to_csv(\n",
    "    \"./output/模型性能汇总_AUC_F1.csv\", index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(f\"模型性能汇总已保存 (包含AUC和F1指标)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PPML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
