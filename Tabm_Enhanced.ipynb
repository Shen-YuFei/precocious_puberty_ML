{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabM增强版：性早熟预测模型\n",
    "\n",
    "**使用TabM进行全方位优化**\n",
    "\n",
    "本笔记本展示了TabM的多种增强技术：\n",
    "- 基础TabM模型（PiecewiseLinearEmbeddings）\n",
    "- 超参数优化（Optuna HPO）\n",
    "- 不同架构变体（tabm / tabm-mini）\n",
    "- 独立批次训练策略\n",
    "- 不同数值嵌入方式对比\n",
    "- SHAP可解释性分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabM版本: 0.0.3\n",
      "PyTorch版本: 2.9.1+cu130\n",
      "CUDA可用: True\n",
      "GPU设备: NVIDIA GeForce RTX 3080 Laptop GPU\n",
      "所有库导入完成\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# TabM相关库\n",
    "import tabm\n",
    "import rtdl_num_embeddings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"SimHei\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "print(f\"TabM版本: {tabm.__version__ if hasattr(tabm, '__version__') else 'N/A'}\")\n",
    "print(f\"PyTorch版本: {torch.__version__}\")\n",
    "print(f\"CUDA可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU设备: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"所有库导入完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 设置路径和参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机种子设置: 825\n",
      "计算设备: cuda\n",
      "输出目录: ./output/tabm_enhanced/\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"./output\", exist_ok=True)\n",
    "os.makedirs(\"./output/models\", exist_ok=True)\n",
    "os.makedirs(\"./output/tabm_enhanced\", exist_ok=True)\n",
    "os.makedirs(\"./output/tabm_enhanced/models\", exist_ok=True)\n",
    "\n",
    "RANDOM_SEED = 825\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"随机种子设置: {RANDOM_SEED}\")\n",
    "print(f\"计算设备: {DEVICE}\")\n",
    "print(f\"输出目录: ./output/tabm_enhanced/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正常组: 8970 行, 早熟组: 10654 行\n"
     ]
    }
   ],
   "source": [
    "normal_data = pd.read_csv(\"./input/性早熟数据激发试验正常组_new.csv\")\n",
    "disease_data = pd.read_csv(\"./input/激发试验确诊性早熟组数据_new.csv\")\n",
    "\n",
    "normal_data[\"group\"] = \"N\"\n",
    "disease_data[\"group\"] = \"Y\"\n",
    "\n",
    "print(f\"正常组: {normal_data.shape[0]} 行, 早熟组: {disease_data.shape[0]} 行\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 数据类型处理和合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并后数据: 19624 行 x 40 列\n",
      "分组统计:\n",
      "group\n",
      "Y    10654\n",
      "N     8970\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat([normal_data, disease_data], axis=0, ignore_index=True)\n",
    "data[\"group\"] = data[\"group\"].astype(\"category\")\n",
    "print(f\"合并后数据: {data.shape[0]} 行 x {data.shape[1]} 列\")\n",
    "print(f\"分组统计:\\n{data['group'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 划分训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集: 13736 行, 验证集: 5888 行\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data = train_test_split(\n",
    "    data, test_size=0.3, stratify=data[\"group\"], random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"训练集: {train_data.shape[0]} 行, 验证集: {validation_data.shape[0]} 行\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用 38 个特征\n",
      "训练集正负样本: {1: 7457, 0: 6279}\n",
      "验证集正负样本: {1: 3197, 0: 2691}\n"
     ]
    }
   ],
   "source": [
    "exclude_cols = [\"group\", \"患者编号\", \"Unnamed: 0\"]\n",
    "feature_cols = [col for col in train_data.columns if col not in exclude_cols]\n",
    "\n",
    "X_train = train_data[feature_cols].copy()\n",
    "y_train = train_data[\"group\"].copy()\n",
    "X_validation = validation_data[feature_cols].copy()\n",
    "y_validation = validation_data[\"group\"].copy()\n",
    "\n",
    "y_train_binary = (y_train == \"Y\").astype(int)\n",
    "y_validation_binary = (y_validation == \"Y\").astype(int)\n",
    "\n",
    "print(f\"使用 {len(feature_cols)} 个特征\")\n",
    "print(f\"训练集正负样本: {y_train_binary.value_counts().to_dict()}\")\n",
    "print(f\"验证集正负样本: {y_validation_binary.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 数据类型转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据类型转换中...\n",
      "转换完成 - 训练集: {dtype('float64'): 38}\n"
     ]
    }
   ],
   "source": [
    "print(\"数据类型转换中...\")\n",
    "\n",
    "for col in feature_cols:\n",
    "    X_train[col] = pd.to_numeric(X_train[col], errors=\"coerce\")\n",
    "    X_validation[col] = pd.to_numeric(X_validation[col], errors=\"coerce\")\n",
    "\n",
    "print(f\"转换完成 - 训练集: {X_train.dtypes.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 数据预处理（缺失值填充）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据预处理开始...\n",
      "预处理完成！\n",
      "  原始特征数: 38\n",
      "  处理后特征数: 38\n",
      "  训练集样本: 13736\n",
      "  验证集样本: 5888\n",
      "  缺失值: 0 (应为0)\n"
     ]
    }
   ],
   "source": [
    "print(\"数据预处理开始...\")\n",
    "\n",
    "# 使用中位数填充缺失值\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "X_train_processed = imputer.fit_transform(X_train)\n",
    "X_validation_processed = imputer.transform(X_validation)\n",
    "\n",
    "# 移除全为NaN的特征\n",
    "valid_features = ~np.isnan(X_train_processed).all(axis=0)\n",
    "X_train_processed = X_train_processed[:, valid_features]\n",
    "X_validation_processed = X_validation_processed[:, valid_features]\n",
    "\n",
    "# 更新特征列表\n",
    "feature_cols_processed = [\n",
    "    col for col, valid in zip(feature_cols, valid_features) if valid\n",
    "]\n",
    "\n",
    "print(f\"预处理完成！\")\n",
    "print(f\"  原始特征数: {X_train.shape[1]}\")\n",
    "print(f\"  处理后特征数: {X_train_processed.shape[1]}\")\n",
    "print(f\"  训练集样本: {X_train_processed.shape[0]}\")\n",
    "print(f\"  验证集样本: {X_validation_processed.shape[0]}\")\n",
    "print(f\"  缺失值: {np.isnan(X_train_processed).sum()} (应为0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. QuantileTransformer预处理（TabM推荐）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "应用QuantileTransformer...\n",
      "训练集张量: torch.Size([13736, 38])\n",
      "验证集张量: torch.Size([5888, 38])\n",
      "QuantileTransformer预处理完成！\n"
     ]
    }
   ],
   "source": [
    "print(\"应用QuantileTransformer...\")\n",
    "\n",
    "# 添加微小噪声以改善QuantileTransformer输出\n",
    "noise = (\n",
    "    np.random.default_rng(RANDOM_SEED)\n",
    "    .normal(0.0, 1e-5, X_train_processed.shape)\n",
    "    .astype(X_train_processed.dtype)\n",
    ")\n",
    "\n",
    "quantile_transformer = QuantileTransformer(\n",
    "    n_quantiles=max(min(len(X_train_processed) // 30, 1000), 10),\n",
    "    output_distribution=\"normal\",\n",
    "    subsample=10**9,\n",
    ").fit(X_train_processed + noise)\n",
    "\n",
    "X_train_transformed = quantile_transformer.transform(X_train_processed)\n",
    "X_validation_transformed = quantile_transformer.transform(X_validation_processed)\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "X_train_tensor = torch.tensor(X_train_transformed, dtype=torch.float32).to(DEVICE)\n",
    "y_train_tensor = torch.tensor(y_train_binary.values, dtype=torch.long).to(DEVICE)\n",
    "X_val_tensor = torch.tensor(X_validation_transformed, dtype=torch.float32).to(DEVICE)\n",
    "y_val_tensor = torch.tensor(y_validation_binary.values, dtype=torch.long).to(DEVICE)\n",
    "\n",
    "print(f\"训练集张量: {X_train_tensor.shape}\")\n",
    "print(f\"验证集张量: {X_val_tensor.shape}\")\n",
    "print(f\"QuantileTransformer预处理完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 模型训练与优化\n",
    "\n",
    "## 10. 定义训练和评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练和评估函数定义完成！\n"
     ]
    }
   ],
   "source": [
    "def train_tabm_model(\n",
    "    model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    n_epochs=500,\n",
    "    batch_size=256,\n",
    "    lr=2e-3,\n",
    "    weight_decay=3e-4,\n",
    "    patience=32,\n",
    "    gradient_clipping_norm=1.0,\n",
    "    share_training_batches=True,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"训练TabM模型的通用函数\"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    amp_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    amp_enabled = torch.cuda.is_available()\n",
    "\n",
    "    best_f1 = 0\n",
    "    best_auc = 0\n",
    "    best_epoch = 0\n",
    "    best_state = None\n",
    "    remaining_patience = patience\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        if share_training_batches:\n",
    "            batches = torch.randperm(len(X_train), device=DEVICE).split(batch_size)\n",
    "        else:\n",
    "            # k个独立批次序列\n",
    "            batches = (\n",
    "                torch.rand((len(X_train), model.backbone.k), device=DEVICE)\n",
    "                .argsort(dim=0)\n",
    "                .split(batch_size, dim=0)\n",
    "            )\n",
    "\n",
    "        for batch_idx in batches:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.autocast(\n",
    "                device_type=\"cuda\", enabled=amp_enabled, dtype=amp_dtype\n",
    "            ):\n",
    "                logits = model(X_train[batch_idx], None)\n",
    "                y_pred = logits.flatten(0, 1)\n",
    "\n",
    "                if share_training_batches:\n",
    "                    y_true = y_train[batch_idx].repeat_interleave(model.backbone.k)\n",
    "                else:\n",
    "                    y_true = y_train[batch_idx].flatten(0, 1)\n",
    "\n",
    "                loss = nn.functional.cross_entropy(y_pred, y_true)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if gradient_clipping_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), gradient_clipping_norm\n",
    "                )\n",
    "\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # 验证\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_logits = model(X_val, None)\n",
    "            val_proba = (\n",
    "                torch.softmax(val_logits, dim=-1).mean(dim=1)[:, 1].cpu().numpy()\n",
    "            )\n",
    "            val_pred = (val_proba >= 0.5).astype(int)\n",
    "\n",
    "            y_val_np = y_val.cpu().numpy()\n",
    "            auc = roc_auc_score(y_val_np, val_proba)\n",
    "            f1 = f1_score(y_val_np, val_pred)\n",
    "\n",
    "        improved = f1 > best_f1\n",
    "\n",
    "        if verbose and (epoch % 50 == 0 or improved):\n",
    "            print(\n",
    "                f\"Epoch {epoch:3d}: Loss={total_loss/len(batches):.4f}, F1={f1:.4f}, AUC={auc:.4f}{' *' if improved else ''}\"\n",
    "            )\n",
    "\n",
    "        if improved:\n",
    "            best_f1, best_auc, best_epoch = f1, auc, epoch\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "            remaining_patience = patience\n",
    "        else:\n",
    "            remaining_patience -= 1\n",
    "\n",
    "        if remaining_patience < 0:\n",
    "            if verbose:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    # 恢复最佳模型\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return {\n",
    "        \"best_f1\": best_f1,\n",
    "        \"best_auc\": best_auc,\n",
    "        \"best_epoch\": best_epoch,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    \"\"\"评估模型性能\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X, None)\n",
    "        proba = torch.softmax(logits, dim=-1).mean(dim=1)[:, 1].cpu().numpy()\n",
    "        pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "        y_np = y.cpu().numpy() if isinstance(y, torch.Tensor) else y\n",
    "\n",
    "        return {\n",
    "            \"auc\": roc_auc_score(y_np, proba),\n",
    "            \"f1\": f1_score(y_np, pred),\n",
    "            \"accuracy\": accuracy_score(y_np, pred),\n",
    "            \"precision\": precision_score(y_np, pred),\n",
    "            \"recall\": recall_score(y_np, pred),\n",
    "            \"y_pred\": pred,\n",
    "            \"y_proba\": proba,\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"训练和评估函数定义完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 模型1：基础TabM（PiecewiseLinearEmbeddings）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "训练基础TabM模型（PiecewiseLinearEmbeddings）\n",
      "======================================================================\n",
      "模型参数量: 738,048\n",
      "集成数量k: 32\n",
      "\n",
      "开始训练...\n",
      "\n",
      "Epoch   0: Loss=0.4282, F1=0.8902, AUC=0.9496 *\n",
      "Epoch   1: Loss=0.2916, F1=0.8968, AUC=0.9589 *\n",
      "Epoch   3: Loss=0.2458, F1=0.9034, AUC=0.9632 *\n",
      "Epoch   4: Loss=0.2361, F1=0.9065, AUC=0.9640 *\n",
      "Epoch   6: Loss=0.2236, F1=0.9098, AUC=0.9661 *\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m集成数量k: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtabm_basic.backbone.k\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m开始训练...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m result_basic = \u001b[43mtrain_tabm_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtabm_basic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_val_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# 最终评估\u001b[39;00m\n\u001b[32m     36\u001b[39m metrics_basic = evaluate_model(tabm_basic, X_val_tensor, y_val_tensor)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mtrain_tabm_model\u001b[39m\u001b[34m(model, X_train, y_train, X_val, y_val, n_epochs, batch_size, lr, weight_decay, patience, gradient_clipping_norm, share_training_batches, verbose)\u001b[39m\n\u001b[32m     59\u001b[39m         torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping_norm)\n\u001b[32m     61\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# 验证\u001b[39;00m\n\u001b[32m     65\u001b[39m model.eval()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"训练基础TabM模型（PiecewiseLinearEmbeddings）\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 创建PiecewiseLinear嵌入\n",
    "num_embeddings_basic = rtdl_num_embeddings.PiecewiseLinearEmbeddings(\n",
    "    rtdl_num_embeddings.compute_bins(X_train_tensor, n_bins=48),\n",
    "    d_embedding=16,\n",
    "    activation=False,\n",
    "    version=\"B\",\n",
    ")\n",
    "\n",
    "# 创建TabM模型\n",
    "tabm_basic = tabm.TabM.make(\n",
    "    n_num_features=X_train_tensor.shape[1],\n",
    "    cat_cardinalities=[],\n",
    "    d_out=2,\n",
    "    num_embeddings=num_embeddings_basic,\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"模型参数量: {sum(p.numel() for p in tabm_basic.parameters()):,}\")\n",
    "print(f\"集成数量k: {tabm_basic.backbone.k}\")\n",
    "print(\"\\n开始训练...\\n\")\n",
    "\n",
    "result_basic = train_tabm_model(\n",
    "    tabm_basic,\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    X_val_tensor,\n",
    "    y_val_tensor,\n",
    "    n_epochs=500,\n",
    "    patience=32,\n",
    ")\n",
    "\n",
    "# 最终评估\n",
    "metrics_basic = evaluate_model(tabm_basic, X_val_tensor, y_val_tensor)\n",
    "\n",
    "print(f\"\\n基础TabM性能:\")\n",
    "print(f\"  AUC: {metrics_basic['auc']:.4f}\")\n",
    "print(f\"  F1:  {metrics_basic['f1']:.4f}\")\n",
    "print(f\"  ACC: {metrics_basic['accuracy']:.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "torch.save(tabm_basic.state_dict(), \"./output/tabm_enhanced/models/tabm_basic.pt\")\n",
    "joblib.dump(\n",
    "    {\"imputer\": imputer, \"quantile_transformer\": quantile_transformer},\n",
    "    \"./output/tabm_enhanced/models/tabm_basic_preprocessors.pkl\",\n",
    ")\n",
    "print(f\"\\n模型已保存: ./output/tabm_enhanced/models/tabm_basic.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 模型2：TabM-Mini架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"训练TabM-Mini架构\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 创建PiecewiseLinear嵌入\n",
    "num_embeddings_mini = rtdl_num_embeddings.PiecewiseLinearEmbeddings(\n",
    "    rtdl_num_embeddings.compute_bins(X_train_tensor, n_bins=48),\n",
    "    d_embedding=16,\n",
    "    activation=False,\n",
    "    version=\"B\",\n",
    ")\n",
    "\n",
    "# 创建TabM-Mini模型\n",
    "tabm_mini = tabm.TabM.make(\n",
    "    n_num_features=X_train_tensor.shape[1],\n",
    "    cat_cardinalities=[],\n",
    "    d_out=2,\n",
    "    num_embeddings=num_embeddings_mini,\n",
    "    arch_type=\"tabm-mini\",  # Mini架构，更强正则化\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"模型参数量: {sum(p.numel() for p in tabm_mini.parameters()):,}\")\n",
    "print(f\"集成数量k: {tabm_mini.backbone.k}\")\n",
    "print(\"\\n开始训练...\\n\")\n",
    "\n",
    "result_mini = train_tabm_model(\n",
    "    tabm_mini,\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    X_val_tensor,\n",
    "    y_val_tensor,\n",
    "    n_epochs=500,\n",
    "    patience=32,\n",
    ")\n",
    "\n",
    "# 最终评估\n",
    "metrics_mini = evaluate_model(tabm_mini, X_val_tensor, y_val_tensor)\n",
    "\n",
    "print(f\"\\nTabM-Mini性能:\")\n",
    "print(f\"  AUC: {metrics_mini['auc']:.4f}\")\n",
    "print(f\"  F1:  {metrics_mini['f1']:.4f}\")\n",
    "print(f\"  ACC: {metrics_mini['accuracy']:.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "torch.save(tabm_mini.state_dict(), \"./output/tabm_enhanced/models/tabm_mini.pt\")\n",
    "print(f\"\\n模型已保存: ./output/tabm_enhanced/models/tabm_mini.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 模型3：独立批次训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"训练TabM（独立批次策略）\")\n",
    "print(\"=\" * 70)\n",
    "print(\"独立批次训练：k个子模型在不同批次上训练，增加多样性\")\n",
    "\n",
    "# 创建PiecewiseLinear嵌入\n",
    "num_embeddings_indep = rtdl_num_embeddings.PiecewiseLinearEmbeddings(\n",
    "    rtdl_num_embeddings.compute_bins(X_train_tensor, n_bins=48),\n",
    "    d_embedding=16,\n",
    "    activation=False,\n",
    "    version=\"B\",\n",
    ")\n",
    "\n",
    "# 创建TabM模型\n",
    "tabm_indep = tabm.TabM.make(\n",
    "    n_num_features=X_train_tensor.shape[1],\n",
    "    cat_cardinalities=[],\n",
    "    d_out=2,\n",
    "    num_embeddings=num_embeddings_indep,\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"模型参数量: {sum(p.numel() for p in tabm_indep.parameters()):,}\")\n",
    "print(\"\\n开始训练...\\n\")\n",
    "\n",
    "result_indep = train_tabm_model(\n",
    "    tabm_indep,\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    X_val_tensor,\n",
    "    y_val_tensor,\n",
    "    n_epochs=500,\n",
    "    patience=32,\n",
    "    share_training_batches=False,  # 独立批次\n",
    ")\n",
    "\n",
    "# 最终评估\n",
    "metrics_indep = evaluate_model(tabm_indep, X_val_tensor, y_val_tensor)\n",
    "\n",
    "print(f\"\\nTabM（独立批次）性能:\")\n",
    "print(f\"  AUC: {metrics_indep['auc']:.4f}\")\n",
    "print(f\"  F1:  {metrics_indep['f1']:.4f}\")\n",
    "print(f\"  ACC: {metrics_indep['accuracy']:.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "torch.save(tabm_indep.state_dict(), \"./output/tabm_enhanced/models/tabm_indep.pt\")\n",
    "print(f\"\\n模型已保存: ./output/tabm_enhanced/models/tabm_indep.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. 模型4：PeriodicEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"训练TabM（PeriodicEmbeddings）\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 创建Periodic嵌入\n",
    "num_embeddings_periodic = rtdl_num_embeddings.PeriodicEmbeddings(\n",
    "    n_features=X_train_tensor.shape[1],\n",
    "    d_embedding=16,\n",
    "    lite=False,\n",
    ")\n",
    "\n",
    "# 创建TabM模型\n",
    "tabm_periodic = tabm.TabM.make(\n",
    "    n_num_features=X_train_tensor.shape[1],\n",
    "    cat_cardinalities=[],\n",
    "    d_out=2,\n",
    "    num_embeddings=num_embeddings_periodic,\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"模型参数量: {sum(p.numel() for p in tabm_periodic.parameters()):,}\")\n",
    "print(\"\\n开始训练...\\n\")\n",
    "\n",
    "result_periodic = train_tabm_model(\n",
    "    tabm_periodic,\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    X_val_tensor,\n",
    "    y_val_tensor,\n",
    "    n_epochs=500,\n",
    "    patience=32,\n",
    ")\n",
    "\n",
    "# 最终评估\n",
    "metrics_periodic = evaluate_model(tabm_periodic, X_val_tensor, y_val_tensor)\n",
    "\n",
    "print(f\"\\nTabM（PeriodicEmbeddings）性能:\")\n",
    "print(f\"  AUC: {metrics_periodic['auc']:.4f}\")\n",
    "print(f\"  F1:  {metrics_periodic['f1']:.4f}\")\n",
    "print(f\"  ACC: {metrics_periodic['accuracy']:.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "torch.save(tabm_periodic.state_dict(), \"./output/tabm_enhanced/models/tabm_periodic.pt\")\n",
    "print(f\"\\n模型已保存: ./output/tabm_enhanced/models/tabm_periodic.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. 模型5：超参数优化（Optuna HPO）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TabM超参数优化（Optuna）\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna目标函数\"\"\"\n",
    "    # 超参数搜索空间\n",
    "    n_blocks = trial.suggest_int(\"n_blocks\", 1, 4)\n",
    "    d_block = trial.suggest_int(\"d_block\", 64, 512, step=64)\n",
    "    n_bins = trial.suggest_int(\"n_bins\", 16, 96, step=16)\n",
    "    d_embedding = trial.suggest_int(\"d_embedding\", 8, 32, step=4)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-1, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.3, step=0.05)\n",
    "\n",
    "    try:\n",
    "        # 创建嵌入\n",
    "        num_embeddings = rtdl_num_embeddings.PiecewiseLinearEmbeddings(\n",
    "            rtdl_num_embeddings.compute_bins(X_train_tensor, n_bins=n_bins),\n",
    "            d_embedding=d_embedding,\n",
    "            activation=False,\n",
    "            version=\"B\",\n",
    "        )\n",
    "\n",
    "        # 创建模型\n",
    "        model = tabm.TabM.make(\n",
    "            n_num_features=X_train_tensor.shape[1],\n",
    "            cat_cardinalities=[],\n",
    "            d_out=2,\n",
    "            num_embeddings=num_embeddings,\n",
    "            n_blocks=n_blocks,\n",
    "            d_block=d_block,\n",
    "            dropout=dropout,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # 训练\n",
    "        result = train_tabm_model(\n",
    "            model,\n",
    "            X_train_tensor,\n",
    "            y_train_tensor,\n",
    "            X_val_tensor,\n",
    "            y_val_tensor,\n",
    "            n_epochs=200,\n",
    "            patience=20,\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        return result[\"best_f1\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Trial failed: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "# 运行优化\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"tabm_hpo\")\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\n最佳参数:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"\\n最佳F1: {study.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用最佳参数训练最终模型\n",
    "print(\"\\n使用最佳参数训练最终模型...\")\n",
    "\n",
    "best_params = study.best_params\n",
    "\n",
    "num_embeddings_hpo = rtdl_num_embeddings.PiecewiseLinearEmbeddings(\n",
    "    rtdl_num_embeddings.compute_bins(X_train_tensor, n_bins=best_params[\"n_bins\"]),\n",
    "    d_embedding=best_params[\"d_embedding\"],\n",
    "    activation=False,\n",
    "    version=\"B\",\n",
    ")\n",
    "\n",
    "tabm_hpo = tabm.TabM.make(\n",
    "    n_num_features=X_train_tensor.shape[1],\n",
    "    cat_cardinalities=[],\n",
    "    d_out=2,\n",
    "    num_embeddings=num_embeddings_hpo,\n",
    "    n_blocks=best_params[\"n_blocks\"],\n",
    "    d_block=best_params[\"d_block\"],\n",
    "    dropout=best_params[\"dropout\"],\n",
    ").to(DEVICE)\n",
    "\n",
    "result_hpo = train_tabm_model(\n",
    "    tabm_hpo,\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    X_val_tensor,\n",
    "    y_val_tensor,\n",
    "    n_epochs=500,\n",
    "    patience=32,\n",
    "    lr=best_params[\"lr\"],\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "# 最终评估\n",
    "metrics_hpo = evaluate_model(tabm_hpo, X_val_tensor, y_val_tensor)\n",
    "\n",
    "print(f\"\\nTabM-HPO性能:\")\n",
    "print(f\"  AUC: {metrics_hpo['auc']:.4f}\")\n",
    "print(f\"  F1:  {metrics_hpo['f1']:.4f}\")\n",
    "print(f\"  ACC: {metrics_hpo['accuracy']:.4f}\")\n",
    "\n",
    "# 保存模型和参数\n",
    "torch.save(tabm_hpo.state_dict(), \"./output/tabm_enhanced/models/tabm_hpo.pt\")\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"best_params\": best_params,\n",
    "        \"imputer\": imputer,\n",
    "        \"quantile_transformer\": quantile_transformer,\n",
    "    },\n",
    "    \"./output/tabm_enhanced/models/tabm_hpo_config.pkl\",\n",
    ")\n",
    "print(f\"\\n模型已保存: ./output/tabm_enhanced/models/tabm_hpo.pt\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. 性能对比汇总"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TabM变体性能对比\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    {\n",
    "        \"模型\": [\n",
    "            \"TabM-Basic\",\n",
    "            \"TabM-Mini\",\n",
    "            \"TabM-IndepBatch\",\n",
    "            \"TabM-Periodic\",\n",
    "            \"TabM-HPO\",\n",
    "        ],\n",
    "        \"AUC\": [\n",
    "            metrics_basic[\"auc\"],\n",
    "            metrics_mini[\"auc\"],\n",
    "            metrics_indep[\"auc\"],\n",
    "            metrics_periodic[\"auc\"],\n",
    "            metrics_hpo[\"auc\"],\n",
    "        ],\n",
    "        \"F1\": [\n",
    "            metrics_basic[\"f1\"],\n",
    "            metrics_mini[\"f1\"],\n",
    "            metrics_indep[\"f1\"],\n",
    "            metrics_periodic[\"f1\"],\n",
    "            metrics_hpo[\"f1\"],\n",
    "        ],\n",
    "        \"Accuracy\": [\n",
    "            metrics_basic[\"accuracy\"],\n",
    "            metrics_mini[\"accuracy\"],\n",
    "            metrics_indep[\"accuracy\"],\n",
    "            metrics_periodic[\"accuracy\"],\n",
    "            metrics_hpo[\"accuracy\"],\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "results_df = results_df.sort_values(\"AUC\", ascending=False)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# 保存结果\n",
    "results_df.to_csv(\"./output/tabm_enhanced/性能对比.csv\", index=False)\n",
    "print(f\"\\n结果已保存: ./output/tabm_enhanced/性能对比.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. ROC曲线对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "models_info = [\n",
    "    (\"TabM-Basic\", metrics_basic[\"y_proba\"], metrics_basic[\"auc\"]),\n",
    "    (\"TabM-Mini\", metrics_mini[\"y_proba\"], metrics_mini[\"auc\"]),\n",
    "    (\"TabM-IndepBatch\", metrics_indep[\"y_proba\"], metrics_indep[\"auc\"]),\n",
    "    (\"TabM-Periodic\", metrics_periodic[\"y_proba\"], metrics_periodic[\"auc\"]),\n",
    "    (\"TabM-HPO\", metrics_hpo[\"y_proba\"], metrics_hpo[\"auc\"]),\n",
    "]\n",
    "\n",
    "y_val_np = y_validation_binary.values\n",
    "\n",
    "for model_name, y_proba, auc_score in models_info:\n",
    "    fpr, tpr, _ = roc_curve(y_val_np, y_proba)\n",
    "    linewidth = 3 if model_name == \"TabM-HPO\" else 2\n",
    "    plt.plot(\n",
    "        fpr, tpr, label=f\"{model_name} (AUC = {auc_score:.4f})\", linewidth=linewidth\n",
    "    )\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", linewidth=1, label=\"随机猜测\")\n",
    "plt.xlabel(\"假阳性率 (1-特异度)\", fontsize=12)\n",
    "plt.ylabel(\"真阳性率 (灵敏度)\", fontsize=12)\n",
    "plt.title(\"TabM变体ROC曲线对比\", fontsize=14, fontweight=\"bold\")\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./output/tabm_enhanced/ROC曲线对比.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(\"./output/tabm_enhanced/ROC曲线对比.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"ROC曲线已保存\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. 选择最佳模型并复制到标准位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"选择最佳模型\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 找出最佳模型\n",
    "all_metrics = {\n",
    "    \"basic\": metrics_basic,\n",
    "    \"mini\": metrics_mini,\n",
    "    \"indep\": metrics_indep,\n",
    "    \"periodic\": metrics_periodic,\n",
    "    \"hpo\": metrics_hpo,\n",
    "}\n",
    "\n",
    "best_model_name = max(all_metrics, key=lambda x: all_metrics[x][\"auc\"])\n",
    "best_metrics = all_metrics[best_model_name]\n",
    "\n",
    "print(f\"最佳模型: TabM-{best_model_name}\")\n",
    "print(f\"  AUC: {best_metrics['auc']:.4f}\")\n",
    "print(f\"  F1:  {best_metrics['f1']:.4f}\")\n",
    "\n",
    "# 复制最佳模型到标准位置\n",
    "import shutil\n",
    "\n",
    "src_model = f\"./output/tabm_enhanced/models/tabm_{best_model_name}.pt\"\n",
    "dst_model = \"./output/models/tabm_best.pt\"\n",
    "shutil.copy(src_model, dst_model)\n",
    "\n",
    "# 保存预处理器\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"imputer\": imputer,\n",
    "        \"quantile_transformer\": quantile_transformer,\n",
    "        \"best_model_name\": best_model_name,\n",
    "        \"metrics\": best_metrics,\n",
    "    },\n",
    "    \"./output/models/tabm_preprocessors.pkl\",\n",
    ")\n",
    "\n",
    "print(f\"\\n最佳模型已复制到: {dst_model}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. 显存使用统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"显存峰值使用: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"当前显存使用: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "    # 清理显存\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"显存已清理\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PPML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
