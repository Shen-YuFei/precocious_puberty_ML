{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabM增强版：性早熟预测模型\n",
    "\n",
    "**使用TabM进行全方位优化**\n",
    "\n",
    "- 基础TabM模型（PiecewiseLinearEmbeddings）\n",
    "- 超参数优化（Optuna HPO）\n",
    "- 不同架构变体（tabm / tabm-mini）\n",
    "- 独立批次训练策略\n",
    "- 不同数值嵌入方式对比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabM版本: 0.0.3\n",
      "PyTorch版本: 2.9.1+cu130\n",
      "CUDA可用: True\n",
      "GPU设备: NVIDIA GeForce RTX 3080 Laptop GPU\n",
      "所有库导入完成\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# TabM相关库\n",
    "import tabm\n",
    "import rtdl_num_embeddings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"SimHei\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "print(f\"TabM版本: {tabm.__version__ if hasattr(tabm, '__version__') else 'N/A'}\")\n",
    "print(f\"PyTorch版本: {torch.__version__}\")\n",
    "print(f\"CUDA可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU设备: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"所有库导入完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 设置路径和参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机种子设置: 825\n",
      "计算设备: cuda\n",
      "输出目录: ./output/tabm_enhanced/\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"./output\", exist_ok=True)\n",
    "os.makedirs(\"./output/models\", exist_ok=True)\n",
    "os.makedirs(\"./output/tabm_enhanced\", exist_ok=True)\n",
    "os.makedirs(\"./output/tabm_enhanced/models\", exist_ok=True)\n",
    "\n",
    "RANDOM_SEED = 825\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"随机种子设置: {RANDOM_SEED}\")\n",
    "print(f\"计算设备: {DEVICE}\")\n",
    "print(f\"输出目录: ./output/tabm_enhanced/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正常组: 299 行, 早熟组: 364 行\n"
     ]
    }
   ],
   "source": [
    "normal_data = pd.read_csv(\"./input/性早熟数据激发试验正常组_new.csv\")\n",
    "disease_data = pd.read_csv(\"./input/激发试验确诊性早熟组数据_new.csv\")\n",
    "\n",
    "normal_data[\"group\"] = \"N\"\n",
    "disease_data[\"group\"] = \"Y\"\n",
    "\n",
    "print(f\"正常组: {normal_data.shape[0]} 行, 早熟组: {disease_data.shape[0]} 行\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 数据类型处理和合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用 38 个特征（排除乳核）\n",
      "分类特征 (4个): ['Tanner分期', '乳晕色素沉着', '有无阴毛', '有无腋毛']\n",
      "数值特征 (34个)\n",
      "\n",
      "使用MissForest方法进行分组填补...\n",
      "正常组填补完成: (299, 38)\n",
      "性早熟组填补完成: (364, 38)\n",
      "\n",
      "合并后数据: 663 行 x 39 列\n",
      "分组统计:\n",
      "group\n",
      "Y    364\n",
      "N    299\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 分组填补：先对每组分别填补，再合并\n",
    "exclude_cols = [\"group\", \"患者编号\", \"Unnamed: 0\"]\n",
    "feature_cols = [col for col in normal_data.columns if col not in exclude_cols]\n",
    "\n",
    "# 定义分类特征和数值特征\n",
    "categorical_info = {\n",
    "    \"Tanner分期\": (1, 5),\n",
    "    \"乳晕色素沉着\": (0, 2),\n",
    "    # \"乳核\": (0, 1),  # 过滤乳腺缺失样本后只有1个唯一值，排除\n",
    "    \"有无阴毛\": (0, 1),\n",
    "    \"有无腋毛\": (0, 1),\n",
    "}\n",
    "categorical_cols = [c for c in categorical_info.keys() if c in feature_cols]\n",
    "numerical_cols = [c for c in feature_cols if c not in categorical_cols and c != \"乳核\"]\n",
    "\n",
    "print(f\"使用 {len(categorical_cols) + len(numerical_cols)} 个特征（排除乳核）\")\n",
    "print(f\"分类特征 ({len(categorical_cols)}个): {categorical_cols}\")\n",
    "print(f\"数值特征 ({len(numerical_cols)}个)\")\n",
    "\n",
    "# ===== 分组填补 =====\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(\"\\n使用MissForest方法进行分组填补...\")\n",
    "\n",
    "# 正常组填补器\n",
    "cat_imputer_normal = IterativeImputer(\n",
    "    estimator=RandomForestClassifier(\n",
    "        n_estimators=10, max_depth=10, n_jobs=-1, random_state=RANDOM_SEED\n",
    "    ),\n",
    "    max_iter=10,\n",
    "    random_state=RANDOM_SEED,\n",
    "    verbose=0,\n",
    ")\n",
    "num_imputer_normal = IterativeImputer(\n",
    "    estimator=RandomForestRegressor(\n",
    "        n_estimators=10, max_depth=10, n_jobs=-1, random_state=RANDOM_SEED\n",
    "    ),\n",
    "    max_iter=10,\n",
    "    random_state=RANDOM_SEED,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# 性早熟组填补器\n",
    "cat_imputer_disease = IterativeImputer(\n",
    "    estimator=RandomForestClassifier(\n",
    "        n_estimators=10, max_depth=10, n_jobs=-1, random_state=RANDOM_SEED\n",
    "    ),\n",
    "    max_iter=10,\n",
    "    random_state=RANDOM_SEED,\n",
    "    verbose=0,\n",
    ")\n",
    "num_imputer_disease = IterativeImputer(\n",
    "    estimator=RandomForestRegressor(\n",
    "        n_estimators=10, max_depth=10, n_jobs=-1, random_state=RANDOM_SEED\n",
    "    ),\n",
    "    max_iter=10,\n",
    "    random_state=RANDOM_SEED,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# 正常组填补\n",
    "normal_cat = (\n",
    "    cat_imputer_normal.fit_transform(normal_data[categorical_cols])\n",
    "    if categorical_cols\n",
    "    else None\n",
    ")\n",
    "normal_num = num_imputer_normal.fit_transform(normal_data[numerical_cols])\n",
    "\n",
    "# 性早熟组填补\n",
    "disease_cat = (\n",
    "    cat_imputer_disease.fit_transform(disease_data[categorical_cols])\n",
    "    if categorical_cols\n",
    "    else None\n",
    ")\n",
    "disease_num = num_imputer_disease.fit_transform(disease_data[numerical_cols])\n",
    "\n",
    "# 裁剪分类特征到有效范围\n",
    "if categorical_cols:\n",
    "    for i, col in enumerate(categorical_cols):\n",
    "        min_val, max_val = categorical_info[col]\n",
    "        normal_cat[:, i] = normal_cat[:, i].clip(min_val, max_val)\n",
    "        disease_cat[:, i] = disease_cat[:, i].clip(min_val, max_val)\n",
    "\n",
    "# 组装填补后的数据\n",
    "if categorical_cols:\n",
    "    normal_imputed = pd.DataFrame(\n",
    "        np.hstack([normal_cat, normal_num]), columns=categorical_cols + numerical_cols\n",
    "    )\n",
    "    disease_imputed = pd.DataFrame(\n",
    "        np.hstack([disease_cat, disease_num]), columns=categorical_cols + numerical_cols\n",
    "    )\n",
    "else:\n",
    "    normal_imputed = pd.DataFrame(normal_num, columns=numerical_cols)\n",
    "    disease_imputed = pd.DataFrame(disease_num, columns=numerical_cols)\n",
    "\n",
    "print(f\"正常组填补完成: {normal_imputed.shape}\")\n",
    "print(f\"性早熟组填补完成: {disease_imputed.shape}\")\n",
    "\n",
    "# 添加标签列\n",
    "normal_imputed[\"group\"] = \"N\"\n",
    "disease_imputed[\"group\"] = \"Y\"\n",
    "\n",
    "# 合并数据\n",
    "data = pd.concat([normal_imputed, disease_imputed], axis=0, ignore_index=True)\n",
    "data[\"group\"] = data[\"group\"].astype(\"category\")\n",
    "print(f\"\\n合并后数据: {data.shape[0]} 行 x {data.shape[1]} 列\")\n",
    "print(f\"分组统计:\\n{data['group'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 划分训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集: 464 行, 验证集: 199 行\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data = train_test_split(\n",
    "    data, test_size=0.3, stratify=data[\"group\"], random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"训练集: {train_data.shape[0]} 行, 验证集: {validation_data.shape[0]} 行\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用 38 个特征（含缺失指示器）\n",
      "训练集正负样本: {1: 255, 0: 209}\n",
      "验证集正负样本: {1: 109, 0: 90}\n"
     ]
    }
   ],
   "source": [
    "exclude_cols = [\"group\", \"患者编号\", \"Unnamed: 0\"]\n",
    "feature_cols = [col for col in train_data.columns if col not in exclude_cols]\n",
    "\n",
    "X_train = train_data[feature_cols].copy()\n",
    "y_train = train_data[\"group\"].copy()\n",
    "X_validation = validation_data[feature_cols].copy()\n",
    "y_validation = validation_data[\"group\"].copy()\n",
    "\n",
    "y_train_binary = (y_train == \"Y\").astype(int)\n",
    "y_validation_binary = (y_validation == \"Y\").astype(int)\n",
    "\n",
    "print(f\"使用 {len(feature_cols)} 个特征（含缺失指示器）\")\n",
    "print(f\"训练集正负样本: {y_train_binary.value_counts().to_dict()}\")\n",
    "print(f\"验证集正负样本: {y_validation_binary.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 数据预处理（缺失值填充）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数值特征已标准化 (StandardScaler)\n",
      "\n",
      "预处理完成！\n",
      "  特征数: 38\n",
      "  训练集样本: 464\n",
      "  验证集样本: 199\n"
     ]
    }
   ],
   "source": [
    "# 数据已在分组填补阶段完成，这里只做标准化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 分离特征（按原始顺序）\n",
    "feature_cols_ordered = categorical_cols + numerical_cols\n",
    "\n",
    "X_train_features = train_data[feature_cols_ordered].values\n",
    "X_validation_features = validation_data[feature_cols_ordered].values\n",
    "\n",
    "# 分类特征保持不变，数值特征标准化\n",
    "cat_count = len(categorical_cols)\n",
    "num_count = len(numerical_cols)\n",
    "\n",
    "if cat_count > 0:\n",
    "    X_train_cat = X_train_features[:, :cat_count]\n",
    "    X_validation_cat = X_validation_features[:, :cat_count]\n",
    "    X_train_num = X_train_features[:, cat_count:]\n",
    "    X_validation_num = X_validation_features[:, cat_count:]\n",
    "else:\n",
    "    X_train_cat = None\n",
    "    X_validation_cat = None\n",
    "    X_train_num = X_train_features\n",
    "    X_validation_num = X_validation_features\n",
    "\n",
    "# 数值特征标准化\n",
    "scaler = StandardScaler()\n",
    "X_train_num_scaled = scaler.fit_transform(X_train_num)\n",
    "X_validation_num_scaled = scaler.transform(X_validation_num)\n",
    "print(\"数值特征已标准化 (StandardScaler)\")\n",
    "\n",
    "# 合并分类和数值特征\n",
    "if cat_count > 0:\n",
    "    X_train_processed = np.hstack([X_train_cat, X_train_num_scaled])\n",
    "    X_validation_processed = np.hstack([X_validation_cat, X_validation_num_scaled])\n",
    "else:\n",
    "    X_train_processed = X_train_num_scaled\n",
    "    X_validation_processed = X_validation_num_scaled\n",
    "\n",
    "feature_cols_processed = feature_cols_ordered\n",
    "\n",
    "print(f\"\\n预处理完成！\")\n",
    "print(f\"  特征数: {X_train_processed.shape[1]}\")\n",
    "print(f\"  训练集样本: {X_train_processed.shape[0]}\")\n",
    "print(f\"  验证集样本: {X_validation_processed.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 数据转换为PyTorch张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换数据为PyTorch张量...\n",
      "训练集张量: torch.Size([464, 38])\n",
      "验证集张量: torch.Size([199, 38])\n",
      "数据转换完成！\n"
     ]
    }
   ],
   "source": [
    "print(\"转换数据为PyTorch张量...\")\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_processed, dtype=torch.float32).to(DEVICE)\n",
    "y_train_tensor = torch.tensor(y_train_binary.values, dtype=torch.long).to(DEVICE)\n",
    "X_val_tensor = torch.tensor(X_validation_processed, dtype=torch.float32).to(DEVICE)\n",
    "y_val_tensor = torch.tensor(y_validation_binary.values, dtype=torch.long).to(DEVICE)\n",
    "\n",
    "print(f\"训练集张量: {X_train_tensor.shape}\")\n",
    "print(f\"验证集张量: {X_val_tensor.shape}\")\n",
    "print(f\"数据转换完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 模型训练与优化\n",
    "\n",
    "## 9. 定义训练和评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练和评估函数定义完成！\n"
     ]
    }
   ],
   "source": [
    "def train_tabm_model(\n",
    "    model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    n_epochs=500,\n",
    "    batch_size=256,\n",
    "    lr=2e-3,\n",
    "    weight_decay=3e-4,\n",
    "    patience=32,\n",
    "    gradient_clipping_norm=1.0,\n",
    "    share_training_batches=True,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"训练TabM模型的通用函数\"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    amp_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    amp_enabled = torch.cuda.is_available()\n",
    "\n",
    "    best_f1 = 0\n",
    "    best_auc = 0\n",
    "    best_epoch = 0\n",
    "    best_state = None\n",
    "    remaining_patience = patience\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        if share_training_batches:\n",
    "            batches = torch.randperm(len(X_train), device=DEVICE).split(batch_size)\n",
    "        else:\n",
    "            # k个独立批次序列\n",
    "            batches = (\n",
    "                torch.rand((len(X_train), model.backbone.k), device=DEVICE)\n",
    "                .argsort(dim=0)\n",
    "                .split(batch_size, dim=0)\n",
    "            )\n",
    "\n",
    "        for batch_idx in batches:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.autocast(\n",
    "                device_type=\"cuda\", enabled=amp_enabled, dtype=amp_dtype\n",
    "            ):\n",
    "                logits = model(X_train[batch_idx], None)\n",
    "                y_pred = logits.flatten(0, 1)\n",
    "\n",
    "                if share_training_batches:\n",
    "                    y_true = y_train[batch_idx].repeat_interleave(model.backbone.k)\n",
    "                else:\n",
    "                    y_true = y_train[batch_idx].flatten(0, 1)\n",
    "\n",
    "                loss = nn.functional.cross_entropy(y_pred, y_true)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if gradient_clipping_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), gradient_clipping_norm\n",
    "                )\n",
    "\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # 验证\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_logits = model(X_val, None)\n",
    "            val_proba = (\n",
    "                torch.softmax(val_logits, dim=-1).mean(dim=1)[:, 1].cpu().numpy()\n",
    "            )\n",
    "            val_pred = (val_proba >= 0.5).astype(int)\n",
    "\n",
    "            y_val_np = y_val.cpu().numpy()\n",
    "            auc = roc_auc_score(y_val_np, val_proba)\n",
    "            f1 = f1_score(y_val_np, val_pred)\n",
    "\n",
    "        improved = f1 > best_f1\n",
    "\n",
    "        if verbose and (epoch % 50 == 0 or improved):\n",
    "            print(\n",
    "                f\"Epoch {epoch:3d}: Loss={total_loss/len(batches):.4f}, F1={f1:.4f}, AUC={auc:.4f}{' *' if improved else ''}\"\n",
    "            )\n",
    "\n",
    "        if improved:\n",
    "            best_f1, best_auc, best_epoch = f1, auc, epoch\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "            remaining_patience = patience\n",
    "        else:\n",
    "            remaining_patience -= 1\n",
    "\n",
    "        if remaining_patience < 0:\n",
    "            if verbose:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    # 恢复最佳模型\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return {\n",
    "        \"best_f1\": best_f1,\n",
    "        \"best_auc\": best_auc,\n",
    "        \"best_epoch\": best_epoch,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    \"\"\"评估模型性能\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X, None)\n",
    "        proba = torch.softmax(logits, dim=-1).mean(dim=1)[:, 1].cpu().numpy()\n",
    "        pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "        y_np = y.cpu().numpy() if isinstance(y, torch.Tensor) else y\n",
    "\n",
    "        return {\n",
    "            \"auc\": roc_auc_score(y_np, proba),\n",
    "            \"f1\": f1_score(y_np, pred),\n",
    "            \"accuracy\": accuracy_score(y_np, pred),\n",
    "            \"precision\": precision_score(y_np, pred),\n",
    "            \"recall\": recall_score(y_np, pred),\n",
    "            \"y_pred\": pred,\n",
    "            \"y_proba\": proba,\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"训练和评估函数定义完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 模型1：基础TabM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "训练基础TabM模型\n",
      "======================================================================\n",
      "模型参数量: 738,048\n",
      "集成数量k: 32\n",
      "\n",
      "开始训练\n",
      "\n",
      "Epoch   0: Loss=0.6807, F1=0.7114, AUC=0.8172 *\n",
      "Epoch   1: Loss=0.6238, F1=0.8083, AUC=0.8345 *\n",
      "Epoch   2: Loss=0.5412, F1=0.8113, AUC=0.8512 *\n",
      "Epoch   3: Loss=0.4605, F1=0.8288, AUC=0.8659 *\n",
      "Epoch   4: Loss=0.4186, F1=0.8357, AUC=0.8886 *\n",
      "Epoch   5: Loss=0.3799, F1=0.8722, AUC=0.9062 *\n",
      "Epoch   6: Loss=0.3425, F1=0.8793, AUC=0.9188 *\n",
      "Epoch   7: Loss=0.3297, F1=0.8811, AUC=0.9256 *\n",
      "Epoch   8: Loss=0.2835, F1=0.8974, AUC=0.9305 *\n",
      "Epoch  10: Loss=0.2461, F1=0.9170, AUC=0.9408 *\n",
      "Epoch  12: Loss=0.2046, F1=0.9177, AUC=0.9513 *\n",
      "Epoch  32: Loss=0.0392, F1=0.9211, AUC=0.9594 *\n",
      "Epoch  50: Loss=0.0048, F1=0.9211, AUC=0.9620\n",
      "Epoch 100: Loss=0.0007, F1=0.9211, AUC=0.9587\n",
      "Epoch 150: Loss=0.0009, F1=0.9163, AUC=0.9583\n",
      "Epoch 165: Loss=0.0004, F1=0.9258, AUC=0.9574 *\n",
      "Epoch 200: Loss=0.0001, F1=0.9163, AUC=0.9604\n",
      "Epoch 211: Loss=0.0002, F1=0.9264, AUC=0.9594 *\n",
      "Epoch 212: Loss=0.0008, F1=0.9304, AUC=0.9591 *\n",
      "Epoch 250: Loss=0.0005, F1=0.9211, AUC=0.9582\n",
      "Epoch 300: Loss=0.0003, F1=0.9163, AUC=0.9534\n",
      "Epoch 350: Loss=0.0000, F1=0.9211, AUC=0.9547\n",
      "Epoch 400: Loss=0.0000, F1=0.9211, AUC=0.9549\n",
      "Epoch 450: Loss=0.0000, F1=0.9163, AUC=0.9558\n",
      "\n",
      "基础TabM性能:\n",
      "  AUC: 0.9591\n",
      "  F1:  0.9304\n",
      "  ACC: 0.9196\n",
      "\n",
      "模型已保存: ./output/tabm_enhanced/models/tabm_basic.pt\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"训练基础TabM模型\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 创建PiecewiseLinear嵌入\n",
    "num_embeddings_basic = rtdl_num_embeddings.PiecewiseLinearEmbeddings(\n",
    "    rtdl_num_embeddings.compute_bins(X_train_tensor, n_bins=48),\n",
    "    d_embedding=16,\n",
    "    activation=False,\n",
    "    version=\"B\",\n",
    ")\n",
    "\n",
    "# 创建TabM模型\n",
    "tabm_basic = tabm.TabM.make(\n",
    "    n_num_features=X_train_tensor.shape[1],\n",
    "    cat_cardinalities=[],\n",
    "    d_out=2,\n",
    "    num_embeddings=num_embeddings_basic,\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"模型参数量: {sum(p.numel() for p in tabm_basic.parameters()):,}\")\n",
    "print(f\"集成数量k: {tabm_basic.backbone.k}\")\n",
    "print(\"\\n开始训练\\n\")\n",
    "\n",
    "n_epochs = 500\n",
    "result_basic = train_tabm_model(\n",
    "    tabm_basic,\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    X_val_tensor,\n",
    "    y_val_tensor,\n",
    "    n_epochs=n_epochs,\n",
    "    patience=n_epochs,\n",
    ")\n",
    "\n",
    "# 最终评估\n",
    "metrics_basic = evaluate_model(tabm_basic, X_val_tensor, y_val_tensor)\n",
    "\n",
    "print(f\"\\n基础TabM性能:\")\n",
    "print(f\"  AUC: {metrics_basic['auc']:.4f}\")\n",
    "print(f\"  F1:  {metrics_basic['f1']:.4f}\")\n",
    "print(f\"  ACC: {metrics_basic['accuracy']:.4f}\")\n",
    "\n",
    "# 保存模型和预处理器\n",
    "torch.save(tabm_basic.state_dict(), \"./output/tabm_enhanced/models/tabm_basic.pt\")\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"cat_imputer_normal\": cat_imputer_normal,\n",
    "        \"cat_imputer_disease\": cat_imputer_disease,\n",
    "        \"num_imputer_normal\": num_imputer_normal,\n",
    "        \"num_imputer_disease\": num_imputer_disease,\n",
    "        \"scaler\": scaler,\n",
    "        \"categorical_cols\": categorical_cols,\n",
    "        \"numerical_cols\": numerical_cols,\n",
    "    },\n",
    "    \"./output/tabm_enhanced/models/tabm_basic_preprocessors.pkl\",\n",
    ")\n",
    "print(f\"\\n模型已保存: ./output/tabm_enhanced/models/tabm_basic.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 模型2：TabM-Mini架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"训练TabM-Mini架构\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 创建PiecewiseLinear嵌入\n",
    "num_embeddings_mini = rtdl_num_embeddings.PiecewiseLinearEmbeddings(\n",
    "    rtdl_num_embeddings.compute_bins(X_train_tensor, n_bins=48),\n",
    "    d_embedding=16,\n",
    "    activation=False,\n",
    "    version=\"B\",\n",
    ")\n",
    "\n",
    "# 创建TabM-Mini模型\n",
    "tabm_mini = tabm.TabM.make(\n",
    "    n_num_features=X_train_tensor.shape[1],\n",
    "    cat_cardinalities=[],\n",
    "    d_out=2,\n",
    "    num_embeddings=num_embeddings_mini,\n",
    "    arch_type=\"tabm-mini\",  # Mini架构，更强正则化\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"模型参数量: {sum(p.numel() for p in tabm_mini.parameters()):,}\")\n",
    "print(f\"集成数量k: {tabm_mini.backbone.k}\")\n",
    "print(\"\\n开始训练...\\n\")\n",
    "\n",
    "n_epochs = 500\n",
    "result_mini = train_tabm_model(\n",
    "    tabm_mini,\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    X_val_tensor,\n",
    "    y_val_tensor,\n",
    "    n_epochs=n_epochs,\n",
    "    patience=n_epochs,\n",
    ")\n",
    "\n",
    "# 最终评估\n",
    "metrics_mini = evaluate_model(tabm_mini, X_val_tensor, y_val_tensor)\n",
    "\n",
    "print(f\"\\nTabM-Mini性能:\")\n",
    "print(f\"  AUC: {metrics_mini['auc']:.4f}\")\n",
    "print(f\"  F1:  {metrics_mini['f1']:.4f}\")\n",
    "print(f\"  ACC: {metrics_mini['accuracy']:.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "torch.save(tabm_mini.state_dict(), \"./output/tabm_enhanced/models/tabm_mini.pt\")\n",
    "print(f\"\\n模型已保存: ./output/tabm_enhanced/models/tabm_mini.pt\")\n",
    "# TabM-Mini性能:\n",
    "#   AUC: 0.8834\n",
    "#   F1:  0.8391\n",
    "#   ACC: 0.8137"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 模型3：独立批次训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"训练TabM（独立批次策略）\")\n",
    "print(\"=\" * 70)\n",
    "print(\"独立批次训练：k个子模型在不同批次上训练，增加多样性\")\n",
    "\n",
    "# 创建PiecewiseLinear嵌入\n",
    "num_embeddings_indep = rtdl_num_embeddings.PiecewiseLinearEmbeddings(\n",
    "    rtdl_num_embeddings.compute_bins(X_train_tensor, n_bins=48),\n",
    "    d_embedding=16,\n",
    "    activation=False,\n",
    "    version=\"B\",\n",
    ")\n",
    "\n",
    "# 创建TabM模型\n",
    "tabm_indep = tabm.TabM.make(\n",
    "    n_num_features=X_train_tensor.shape[1],\n",
    "    cat_cardinalities=[],\n",
    "    d_out=2,\n",
    "    num_embeddings=num_embeddings_indep,\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"模型参数量: {sum(p.numel() for p in tabm_indep.parameters()):,}\")\n",
    "print(\"\\n开始训练...\\n\")\n",
    "\n",
    "n_epochs = 500\n",
    "result_indep = train_tabm_model(\n",
    "    tabm_indep,\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    X_val_tensor,\n",
    "    y_val_tensor,\n",
    "    n_epochs=n_epochs,\n",
    "    patience=n_epochs,\n",
    "    share_training_batches=False,\n",
    ")\n",
    "\n",
    "# 最终评估\n",
    "metrics_indep = evaluate_model(tabm_indep, X_val_tensor, y_val_tensor)\n",
    "\n",
    "print(f\"\\nTabM（独立批次）性能:\")\n",
    "print(f\"  AUC: {metrics_indep['auc']:.4f}\")\n",
    "print(f\"  F1:  {metrics_indep['f1']:.4f}\")\n",
    "print(f\"  ACC: {metrics_indep['accuracy']:.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "torch.save(tabm_indep.state_dict(), \"./output/tabm_enhanced/models/tabm_indep.pt\")\n",
    "print(f\"\\n模型已保存: ./output/tabm_enhanced/models/tabm_indep.pt\")\n",
    "# TabM（独立批次）性能:\n",
    "#   AUC: 0.8866\n",
    "#   F1:  0.8344\n",
    "#   ACC: 0.8089"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 模型4：PeriodicEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"训练TabM（PeriodicEmbeddings）\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 创建Periodic嵌入\n",
    "num_embeddings_periodic = rtdl_num_embeddings.PeriodicEmbeddings(\n",
    "    n_features=X_train_tensor.shape[1],\n",
    "    d_embedding=16,\n",
    "    lite=False,\n",
    ")\n",
    "\n",
    "# 创建TabM模型\n",
    "tabm_periodic = tabm.TabM.make(\n",
    "    n_num_features=X_train_tensor.shape[1],\n",
    "    cat_cardinalities=[],\n",
    "    d_out=2,\n",
    "    num_embeddings=num_embeddings_periodic,\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"模型参数量: {sum(p.numel() for p in tabm_periodic.parameters()):,}\")\n",
    "print(\"\\n开始训练...\\n\")\n",
    "\n",
    "n_epochs = 500\n",
    "result_periodic = train_tabm_model(\n",
    "    tabm_periodic,\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    X_val_tensor,\n",
    "    y_val_tensor,\n",
    "    n_epochs=n_epochs,\n",
    "    patience=n_epochs,\n",
    ")\n",
    "\n",
    "# 最终评估\n",
    "metrics_periodic = evaluate_model(tabm_periodic, X_val_tensor, y_val_tensor)\n",
    "\n",
    "print(f\"\\nTabM（PeriodicEmbeddings）性能:\")\n",
    "print(f\"  AUC: {metrics_periodic['auc']:.4f}\")\n",
    "print(f\"  F1:  {metrics_periodic['f1']:.4f}\")\n",
    "print(f\"  ACC: {metrics_periodic['accuracy']:.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "torch.save(tabm_periodic.state_dict(), \"./output/tabm_enhanced/models/tabm_periodic.pt\")\n",
    "print(f\"\\n模型已保存: ./output/tabm_enhanced/models/tabm_periodic.pt\")\n",
    "# TabM（PeriodicEmbeddings）性能:\n",
    "#   AUC: 0.8710\n",
    "#   F1:  0.8255\n",
    "#   ACC: 0.7945"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. 模型5：超参数优化（Optuna HPO）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TabM超参数优化（Optuna）\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 全局变量保存最佳模型\n",
    "best_hpo_model_state = None\n",
    "best_hpo_f1 = 0.0\n",
    "best_hpo_config = None\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna目标函数\"\"\"\n",
    "    global best_hpo_model_state, best_hpo_f1, best_hpo_config\n",
    "\n",
    "    # 超参数搜索空间\n",
    "    n_blocks = trial.suggest_int(\"n_blocks\", 1, 4)\n",
    "    d_block = trial.suggest_int(\"d_block\", 64, 512, step=64)\n",
    "    n_bins = trial.suggest_int(\"n_bins\", 16, 96, step=16)\n",
    "    d_embedding = trial.suggest_int(\"d_embedding\", 8, 32, step=4)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-1, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.3, step=0.05)\n",
    "\n",
    "    try:\n",
    "        # 创建嵌入\n",
    "        num_embeddings = rtdl_num_embeddings.PiecewiseLinearEmbeddings(\n",
    "            rtdl_num_embeddings.compute_bins(X_train_tensor, n_bins=n_bins),\n",
    "            d_embedding=d_embedding,\n",
    "            activation=False,\n",
    "            version=\"B\",\n",
    "        )\n",
    "\n",
    "        # 创建模型\n",
    "        model = tabm.TabM.make(\n",
    "            n_num_features=X_train_tensor.shape[1],\n",
    "            cat_cardinalities=[],\n",
    "            d_out=2,\n",
    "            num_embeddings=num_embeddings,\n",
    "            n_blocks=n_blocks,\n",
    "            d_block=d_block,\n",
    "            dropout=dropout,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # 训练\n",
    "        n_epochs = 200\n",
    "        result = train_tabm_model(\n",
    "            model,\n",
    "            X_train_tensor,\n",
    "            y_train_tensor,\n",
    "            X_val_tensor,\n",
    "            y_val_tensor,\n",
    "            n_epochs=n_epochs,\n",
    "            patience=50,  # 早停轮数\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        f1 = result[\"best_f1\"]\n",
    "\n",
    "        # 如果是新的最佳模型，保存权重\n",
    "        if f1 > best_hpo_f1:\n",
    "            best_hpo_f1 = f1\n",
    "            best_hpo_model_state = deepcopy(model.state_dict())\n",
    "            best_hpo_config = {\n",
    "                \"n_blocks\": n_blocks,\n",
    "                \"d_block\": d_block,\n",
    "                \"n_bins\": n_bins,\n",
    "                \"d_embedding\": d_embedding,\n",
    "                \"dropout\": dropout,\n",
    "            }\n",
    "\n",
    "        return f1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Trial failed: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "# 运行优化\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"tabm_hpo\")\n",
    "study.optimize(objective, n_trials=1000, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\n最佳参数:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"\\n最佳F1: {study.best_value:.4f}\")\n",
    "\n",
    "# 保存HPO过程中的最佳模型权重\n",
    "torch.save(best_hpo_model_state, \"./output/tabm_enhanced/models/tabm_hpo.pt\")\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"best_params\": study.best_params,\n",
    "        \"best_config\": best_hpo_config,\n",
    "        \"cat_imputer_normal\": cat_imputer_normal,\n",
    "        \"cat_imputer_disease\": cat_imputer_disease,\n",
    "        \"num_imputer_normal\": num_imputer_normal,\n",
    "        \"num_imputer_disease\": num_imputer_disease,\n",
    "        \"scaler\": scaler,\n",
    "        \"categorical_cols\": categorical_cols,\n",
    "        \"numerical_cols\": numerical_cols,\n",
    "    },\n",
    "    \"./output/tabm_enhanced/models/tabm_hpo_config.pkl\",\n",
    ")\n",
    "print(f\"\\nHPO最佳模型已保存: ./output/tabm_enhanced/models/tabm_hpo.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型导入\n",
    "print(\"=\" * 70)\n",
    "print(\"导入已保存的TabM模型\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 加载预处理器（包含scaler）\n",
    "preprocessors = joblib.load(\n",
    "    \"./output/tabm_enhanced/models/tabm_basic_preprocessors.pkl\"\n",
    ")\n",
    "imputer = preprocessors[\"imputer\"]\n",
    "scaler = preprocessors.get(\"scaler\", None)\n",
    "if scaler:\n",
    "    print(\"已加载StandardScaler\")\n",
    "\n",
    "# 模型配置\n",
    "model_configs = {\n",
    "    \"basic\": {\"arch_type\": \"tabm\", \"n_bins\": 48, \"d_embedding\": 16},\n",
    "    \"mini\": {\"arch_type\": \"tabm-mini\", \"n_bins\": 48, \"d_embedding\": 16},\n",
    "    \"indep\": {\"arch_type\": \"tabm\", \"n_bins\": 48, \"d_embedding\": 16},\n",
    "    \"periodic\": {\"arch_type\": \"tabm\", \"use_periodic\": True, \"d_embedding\": 16},\n",
    "}\n",
    "\n",
    "# 尝试加载HPO配置\n",
    "hpo_config_path = \"./output/tabm_enhanced/models/tabm_hpo_config.pkl\"\n",
    "if os.path.exists(hpo_config_path):\n",
    "    hpo_data = joblib.load(hpo_config_path)\n",
    "    best_params = hpo_data[\"best_params\"]\n",
    "    best_hpo_config = hpo_data.get(\"best_config\", best_params)\n",
    "    model_configs[\"hpo\"] = {\n",
    "        \"arch_type\": \"tabm\",\n",
    "        \"n_bins\": best_hpo_config[\"n_bins\"],\n",
    "        \"d_embedding\": best_hpo_config[\"d_embedding\"],\n",
    "        \"n_blocks\": best_hpo_config[\"n_blocks\"],\n",
    "        \"d_block\": best_hpo_config[\"d_block\"],\n",
    "        \"dropout\": best_hpo_config[\"dropout\"],\n",
    "    }\n",
    "    print(f\"HPO最佳参数: {best_params}\")\n",
    "\n",
    "# 加载所有模型\n",
    "loaded_models = {}\n",
    "all_metrics = {}\n",
    "\n",
    "for name, config in model_configs.items():\n",
    "    model_path = f\"./output/tabm_enhanced/models/tabm_{name}.pt\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"  {name}: 模型文件不存在，跳过\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # 创建嵌入层\n",
    "        if config.get(\"use_periodic\", False):\n",
    "            num_embeddings = rtdl_num_embeddings.PeriodicEmbeddings(\n",
    "                n_features=X_train_tensor.shape[1],\n",
    "                d_embedding=config[\"d_embedding\"],\n",
    "                lite=False,\n",
    "            )\n",
    "        else:\n",
    "            num_embeddings = rtdl_num_embeddings.PiecewiseLinearEmbeddings(\n",
    "                rtdl_num_embeddings.compute_bins(\n",
    "                    X_train_tensor, n_bins=config[\"n_bins\"]\n",
    "                ),\n",
    "                d_embedding=config[\"d_embedding\"],\n",
    "                activation=False,\n",
    "                version=\"B\",\n",
    "            )\n",
    "\n",
    "        # 创建模型\n",
    "        model_kwargs = {\n",
    "            \"n_num_features\": X_train_tensor.shape[1],\n",
    "            \"cat_cardinalities\": [],\n",
    "            \"d_out\": 2,\n",
    "            \"num_embeddings\": num_embeddings,\n",
    "        }\n",
    "        if config[\"arch_type\"] == \"tabm-mini\":\n",
    "            model_kwargs[\"arch_type\"] = \"tabm-mini\"\n",
    "        if \"n_blocks\" in config:\n",
    "            model_kwargs[\"n_blocks\"] = config[\"n_blocks\"]\n",
    "        if \"d_block\" in config:\n",
    "            model_kwargs[\"d_block\"] = config[\"d_block\"]\n",
    "        if \"dropout\" in config:\n",
    "            model_kwargs[\"dropout\"] = config[\"dropout\"]\n",
    "\n",
    "        model = tabm.TabM.make(**model_kwargs).to(DEVICE)\n",
    "        model.load_state_dict(\n",
    "            torch.load(model_path, map_location=DEVICE, weights_only=True)\n",
    "        )\n",
    "        model.eval()\n",
    "\n",
    "        # 评估模型\n",
    "        metrics = evaluate_model(model, X_val_tensor, y_val_tensor)\n",
    "\n",
    "        loaded_models[name] = model\n",
    "        all_metrics[name] = metrics\n",
    "\n",
    "        print(f\"  TabM-{name}: F1={metrics['f1']:.4f}, AUC={metrics['auc']:.4f} ✓\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  {name}: 加载失败 - {e}\")\n",
    "\n",
    "# 为后续代码准备变量\n",
    "if \"basic\" in loaded_models:\n",
    "    tabm_basic = loaded_models[\"basic\"]\n",
    "    metrics_basic = all_metrics[\"basic\"]\n",
    "if \"mini\" in loaded_models:\n",
    "    tabm_mini = loaded_models[\"mini\"]\n",
    "    metrics_mini = all_metrics[\"mini\"]\n",
    "if \"indep\" in loaded_models:\n",
    "    tabm_indep = loaded_models[\"indep\"]\n",
    "    metrics_indep = all_metrics[\"indep\"]\n",
    "if \"periodic\" in loaded_models:\n",
    "    tabm_periodic = loaded_models[\"periodic\"]\n",
    "    metrics_periodic = all_metrics[\"periodic\"]\n",
    "if \"hpo\" in loaded_models:\n",
    "    tabm_hpo = loaded_models[\"hpo\"]\n",
    "    metrics_hpo = all_metrics[\"hpo\"]\n",
    "\n",
    "print(f\"\\n成功导入 {len(loaded_models)} 个模型\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TabM变体性能对比\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    {\n",
    "        \"模型\": [\n",
    "            \"TabM-Basic\",\n",
    "            \"TabM-Mini\",\n",
    "            \"TabM-IndepBatch\",\n",
    "            \"TabM-Periodic\",\n",
    "            \"TabM-HPO\",\n",
    "        ],\n",
    "        \"AUC\": [\n",
    "            metrics_basic[\"auc\"],\n",
    "            metrics_mini[\"auc\"],\n",
    "            metrics_indep[\"auc\"],\n",
    "            metrics_periodic[\"auc\"],\n",
    "            metrics_hpo[\"auc\"],\n",
    "        ],\n",
    "        \"F1\": [\n",
    "            metrics_basic[\"f1\"],\n",
    "            metrics_mini[\"f1\"],\n",
    "            metrics_indep[\"f1\"],\n",
    "            metrics_periodic[\"f1\"],\n",
    "            metrics_hpo[\"f1\"],\n",
    "        ],\n",
    "        \"Accuracy\": [\n",
    "            metrics_basic[\"accuracy\"],\n",
    "            metrics_mini[\"accuracy\"],\n",
    "            metrics_indep[\"accuracy\"],\n",
    "            metrics_periodic[\"accuracy\"],\n",
    "            metrics_hpo[\"accuracy\"],\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "results_df = results_df.sort_values(\"AUC\", ascending=False)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# 保存结果\n",
    "results_df.to_csv(\"./output/tabm_enhanced/性能对比.csv\", index=False)\n",
    "print(f\"\\n结果已保存: ./output/tabm_enhanced/性能对比.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. ROC曲线对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "models_info = [\n",
    "    (\"TabM-Basic\", metrics_basic[\"y_proba\"], metrics_basic[\"auc\"]),\n",
    "    (\"TabM-Mini\", metrics_mini[\"y_proba\"], metrics_mini[\"auc\"]),\n",
    "    (\"TabM-IndepBatch\", metrics_indep[\"y_proba\"], metrics_indep[\"auc\"]),\n",
    "    (\"TabM-Periodic\", metrics_periodic[\"y_proba\"], metrics_periodic[\"auc\"]),\n",
    "    (\"TabM-HPO\", metrics_hpo[\"y_proba\"], metrics_hpo[\"auc\"]),\n",
    "]\n",
    "\n",
    "y_val_np = y_validation_binary.values\n",
    "\n",
    "for model_name, y_proba, auc_score in models_info:\n",
    "    fpr, tpr, _ = roc_curve(y_val_np, y_proba)\n",
    "    linewidth = 3 if model_name == \"TabM-HPO\" else 2\n",
    "    plt.plot(\n",
    "        fpr, tpr, label=f\"{model_name} (AUC = {auc_score:.4f})\", linewidth=linewidth\n",
    "    )\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", linewidth=1, label=\"随机猜测\")\n",
    "plt.xlabel(\"假阳性率 (1-特异度)\", fontsize=12)\n",
    "plt.ylabel(\"真阳性率 (灵敏度)\", fontsize=12)\n",
    "plt.title(\"TabM变体ROC曲线对比\", fontsize=14, fontweight=\"bold\")\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./output/tabm_enhanced/ROC曲线对比.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(\"./output/tabm_enhanced/ROC曲线对比.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"ROC曲线已保存\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. 选择最佳模型并复制到标准位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"选择最佳模型（以F1为标准）\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 找出最佳模型\n",
    "all_metrics = {\n",
    "    \"basic\": metrics_basic,\n",
    "    \"mini\": metrics_mini,\n",
    "    \"indep\": metrics_indep,\n",
    "    \"periodic\": metrics_periodic,\n",
    "    \"hpo\": metrics_hpo,\n",
    "}\n",
    "\n",
    "best_model_name = max(all_metrics, key=lambda x: all_metrics[x][\"f1\"])  # 以F1为标准\n",
    "best_metrics = all_metrics[best_model_name]\n",
    "\n",
    "print(f\"最佳模型: TabM-{best_model_name}\")\n",
    "print(f\"  F1:  {best_metrics['f1']:.4f}\")\n",
    "print(f\"  AUC: {best_metrics['auc']:.4f}\")\n",
    "\n",
    "# 复制最佳模型到标准位置\n",
    "import shutil\n",
    "\n",
    "src_model = f\"./output/tabm_enhanced/models/tabm_{best_model_name}.pt\"\n",
    "dst_model = \"./output/models/tabm_best.pt\"\n",
    "shutil.copy(src_model, dst_model)\n",
    "\n",
    "# 保存预处理器（包含scaler用于标准化）\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"imputer\": imputer,\n",
    "        \"scaler\": scaler,\n",
    "        \"best_model_name\": best_model_name,\n",
    "        \"metrics\": best_metrics,\n",
    "    },\n",
    "    \"./output/models/tabm_preprocessors.pkl\",\n",
    ")\n",
    "\n",
    "print(f\"\\n最佳模型已复制到: {dst_model}\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PPML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
